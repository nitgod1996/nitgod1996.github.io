



<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="../images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="宁理大神1996" href="https://nitgod1996.com/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="宁理大神1996" href="https://nitgod1996.com/atom.xml" />
<link rel="alternate" type="application/json" title="宁理大神1996" href="https://nitgod1996.com/feed.json" />



<link rel="stylesheet" href="../css/app.css?v=0.2.5">

  
  <meta name="keywords" content="算法,Python,机器学习" />


<link rel="canonical" href="https://nitgod1996.com/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">



  <title>
西瓜书复习及其部分代码实现 - 笔记 |
nitgod1996 = 宁理大神 1996</title>
<meta name="generator" content="Hexo 5.4.0"><link rel="stylesheet" href="/css/prism.css" type="text/css"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">西瓜书复习及其部分代码实现
  </h1>
  
<div class="meta">
  <span class="item" title="创建时间：2021-06-10 19:20:23">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">发表于</span>
    <time itemprop="dateCreated datePublished" datetime="2021-06-10T19:20:23+08:00">2021-06-10</time>
  </span>
  <span class="item" title="本文字数">
    <span class="icon">
      <i class="ic i-pen"></i>
    </span>
    <span class="text">本文字数</span>
    <span>33k</span>
    <span class="text">字</span>
  </span>
  <span class="item" title="阅读时长">
    <span class="icon">
      <i class="ic i-clock"></i>
    </span>
    <span class="text">阅读时长</span>
    <span>30 分钟</span>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="切换导航栏">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">nitgod1996</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giciusoyjnj219g0u0x56.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gicitcxhpij20zk0m8hdt.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gicit31ffoj20zk0m8naf.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclhtuo6nj20zk0m8ttm.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giciryrr3rj20zk0m8nhk.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gicit4jrvuj20zk0m8785.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="../../../../index.html">首页</a></span><i class="ic i-angle-right"></i>
<span  class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="../../../../categories/note/" itemprop="item" rel="index" title="分类于 笔记"><span itemprop="name">笔记</span></a>
<meta itemprop="position" content="1" /></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN">
  <link itemprop="mainEntityOfPage" href="https://nitgod1996.com/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="../../../../images/avatar.jpg">
    <meta itemprop="name" content="宁理大神1996">
    <meta itemprop="description" content=", 宁理大神的个人博客">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="宁理大神 1996">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <h1 id="数学符号"><a class="anchor" href="#数学符号">#</a> 数学符号</h1>
<ul>
<li><code>argmin</code> ：表示函数取最小值时自变量的集合</li>
</ul>
<h1 id="一-绪论"><a class="anchor" href="#一-绪论">#</a> 一、 绪论</h1>
<h2 id="1-基本术语及概念"><a class="anchor" href="#1-基本术语及概念">#</a> 1. 基本术语及概念</h2>
<ul>
<li><strong>特征 (feature)</strong>：样本的某个<strong>属性</strong>。</li>
<li><strong>属性空间 / 样本空间</strong>：一条属性是一个<strong>维度</strong>，n 条属性看作是<strong> n 维空间</strong>。每个样本可以在该空间找到自己的<strong>坐标</strong></li>
<li><strong>维数</strong>：样本的特征 / 属性数</li>
<li><strong>数据集 (dataSet)</strong>：一组样本</li>
<li><strong>标记</strong>：训练样本的<strong>结果</strong>。如西瓜是好瓜还是坏瓜。</li>
<li><strong>样例 (example)</strong>：拥有标记信息的示例</li>
<li><strong>分类 (classificaation)/ 回归 (regression)</strong>：预测<strong>离散值</strong>为分类，如好瓜、坏瓜；预测<strong>连续值</strong>为回归，如成熟度 0.95</li>
<li><strong>最小二乘</strong>：最小二乘法是一种数学优化技术，它通过<strong>最小化误差的平方和</strong>找到一组数据的<strong>最佳函数</strong>匹配。
<ul>
<li>几何意义：求一条线使得所有点到该线的<strong>距离平方和</strong>最小</li>
</ul>
</li>
<li><strong>权重 ω：<strong>代表各参数的</strong>重要性</strong></li>
<li><strong>偏置 b：<strong>在神经网络中表示该神经元被激活的</strong>容易程度</strong>；</li>
<li>** 激活函数：** 神经网络中，将每层输入信号 (a=wx+b) 转换为输出信号，其作用是决定该神经元是否被激活</li>
</ul>
<h2 id="2-激活函数"><a class="anchor" href="#2-激活函数">#</a> 2. 激活函数</h2>
<p>有下激活函数所示，绝大多数激活函数在特定范围为 0，即不被激活，也就意味着该神经元对后序网络没有信息传递</p>
<h3 id="21-阶跃函数"><a class="anchor" href="#21-阶跃函数">#</a> 2.1 阶跃函数</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220755046.png" alt="image-20210621220755046"></p>
<h3 id="22-sigmoid-函数"><a class="anchor" href="#22-sigmoid函数">#</a> 2.2 sigmoid 函数</h3>
<p>​     <img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220802439.png" alt="image-20210621220802439"></p>
<h3 id="23-relu-函数"><a class="anchor" href="#23-relu函数">#</a> 2.3 ReLU 函数</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220811079.png" alt="image-20210621220811079"></p>
<h3 id="24-softmax-函数最后一层输出层的激活函数"><a class="anchor" href="#24-softmax函数最后一层输出层的激活函数">#</a> 2.4 softmax 函数（最后一层输出层的激活函数）</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220818680.png" alt="image-20210621220818680"></p>
<h3 id="25-函数图"><a class="anchor" href="#25-函数图">#</a> 2.5 函数图</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220826903.png" alt="image-20210621220826903"> 其中：深蓝色的是 relu，虚线是阶跃函数，蓝绿色的是 sigmod 函数</p>
<h2 id="3-梯度下降法"><a class="anchor" href="#3-梯度下降法">#</a> 3. 梯度下降法</h2>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210710171023412.png" alt="image-20210710171023412">θ 是属性集合，在一维函数是 x，二维函数一般是 x、y</p>
<p>因为是往梯度负方向走，所以是减法</p>
<h2 id="梯度"><a class="anchor" href="#梯度">#</a> 梯度</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbm9sdXllL3AvMTExMDg1MTMuaHRtbA==">https://www.cnblogs.com/noluye/p/11108513.html</span></p>
<h2 id="缺点"><a class="anchor" href="#缺点">#</a> 缺点：</h2>
<p>​    可能会因为梯度过小，无法收敛，如下图 (x,y)=(3,-3) 时</p>
<h2 id="代码实现"><a class="anchor" href="#代码实现">#</a> 代码实现：</h2>
<p>以 f (x,y)=−e−(x2+y2) 为例</p>
<h4 id="1-定义目标函数"><a class="anchor" href="#1-定义目标函数">#</a> 1. 定义目标函数</h4>
<pre><code>def func_2d(x):
    :param x: 自变量，一维数组x[]
    :return: 因变量，标量y
    return - math.exp(-(x[0] ** 2 + x[1] ** 2))
</code></pre>
<h4 id="2-求出梯度函数"><a class="anchor" href="#2-求出梯度函数">#</a> 2. 求出梯度函数</h4>
<pre><code>def grad_2d(x):
    :param x: 自变量，二维向量
    :return: 因变量，二维向量
    deriv0 = 2 * x[0] * math.exp(-(x[0] ** 2 + x[1] ** 2))
    deriv1 = 2 * x[1] * math.exp(-(x[0] ** 2 + x[1] ** 2))
    return np.array([deriv0, deriv1])    
 
</code></pre>
<h4 id="3-实现梯度下降法"><a class="anchor" href="#3-实现梯度下降法">#</a> 3. 实现梯度下降法</h4>
<pre><code>def gradient_descent_2d(grad, cur_x=np.array([0.1, 0.1]), learning_rate=0.01, precision=0.0001, max_iters=10000):
    二维问题的梯度下降法
    :param grad: 目标函数的梯度，以函数传参
    :param cur_x: 起始点，通过参数可以提供初始值
    :param learning_rate: 学习率，也相当于设置的步长（上式α）
    :param precision: 设置收敛精度
    :param max_iters: 最大迭代次数
    :return: 局部最小值 x*
    for i in range(max_iters):
        grad_cur = grad(cur_x) #更新梯度
        if np.linalg.norm(grad_cur, ord=2) &lt; precision:
            break  # 当梯度趋近为 0 时，视为收敛
        cur_x = cur_x - grad_cur * learning_rate#迭代自变量
 
公式：
 

        print(&quot;第&quot;, i, &quot;次迭代：x 值为 &quot;, cur_x)

    print(&quot;局部最小值 x =&quot;, cur_x)
    return cur_x
</code></pre>
<h1 id="二-模型评估与选择"><a class="anchor" href="#二-模型评估与选择">#</a> 二、 模型评估与选择</h1>
<h2 id="1-误差与过拟合"><a class="anchor" href="#1-误差与过拟合">#</a> 1. 误差与过拟合</h2>
<ul>
<li><strong>训练误差 / 经验误差</strong>：学习器在训练集上的误差</li>
<li><strong>泛化误差</strong>：在<strong>新样本</strong>（不知道的样本）上的误差</li>
</ul>
<p>在建立学习器时，我们往往使用一组样本，将其分为训练样本和测试样本，训练和测试用的是同一样本。</p>
<p>精度 accuracy 是学习器预测结果相较于测试样本标记的正确率，当测试样本足够大时，精确到可能能到 100%，但会造成<strong>过拟合</strong>。</p>
<p>因为我们希望是学习器在<strong>新样本</strong>上的预测效果好，而非是训练和测试的样本</p>
<ul>
<li><strong>过拟合</strong>：对训练样本<strong>非一般</strong>的特征学习，导致泛化能力不足。</li>
</ul>
<h2 id="2-评估方法"><a class="anchor" href="#2-评估方法">#</a> 2. 评估方法</h2>
<p>我们对学习器的<strong>泛化误差</strong>进行评估，以此来评价这个学习器的好坏</p>
<h1 id="三-线性模型"><a class="anchor" href="#三-线性模型">#</a> 三、 线性模型</h1>
<blockquote>
<p>这里是开始的重点，主要是线性回归和逻辑回归</p>
</blockquote>
<ul>
<li><strong>线性模型</strong>：通过属性的线性组合来进行预测<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msub><mi>w</mi><mi>d</mi></msub><msub><mi>x</mi><mi>d</mi></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">f(x)=w_1x_1+w_2x_2+...+w_dx_d+b
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p>
用向量形式，写成<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi mathvariant="bold-italic">w</mi><mi>T</mi></msup><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9746609999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p>
其中， <code>x</code> ：属性， <code>w</code> ：属性<strong>权重</strong>， <code>b</code> ：偏移</li>
</ul>
<p>因此，只需<strong>求出 <code>w=(w1,w2..)</code>  和 <code>b</code> </strong>，模型即可确定。而几何意义上看是一条直线把样本分成了 2 个阵营（二分类）</p>
<ul>
<li><strong>最小二乘</strong>：最小二乘法是一种数学优化技术，它通过<strong>最小化误差的平方和</strong>找到一组数据的<strong>最佳函数</strong>匹配。
<ul>
<li>几何意义：求一条线使得所有点到该线的<strong>距离平方和</strong>最小</li>
</ul>
</li>
</ul>
<h2 id="1-线性回归linear-regression"><a class="anchor" href="#1-线性回归linear-regression">#</a> 1. 线性回归（linear regression）</h2>
<p>回归一般用于预测<strong>连续值</strong></p>
<h3 id="11-算法原理"><a class="anchor" href="#11-算法原理">#</a> 1.1 算法原理</h3>
<p>试图通过学得一个<strong>线性模型</strong>来预测数据</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi mathvariant="bold-italic">w</mi><mi>T</mi></msup><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi>b</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1.1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b\tag{1.1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9746609999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span><span class="tag"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span><span class="mord">.</span><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p>
<blockquote>
<p>所以，只需求出<strong>权重 w</strong> 和<strong>偏移 b</strong>，即可得出线性模型</p>
</blockquote>
<h4 id="111-单属性线性回归"><a class="anchor" href="#111-单属性线性回归">#</a> 1.1.1 单属性线性回归</h4>
<p>当每条样本仅有一个属性时：</p>
<p>对于每条样本数据的预测，即为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo separator="true">,</mo><mtext>使得</mtext><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>≃</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1.2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">f({x_i})={w_i}{x_i}+b,使得f(x_i)\simeq y_i\tag{1.2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">使</span><span class="mord cjk_fallback">得</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≃</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中， <code>f(x)</code>  是是样本的<strong>预测数据</strong>。 <code>yi</code>  是样本的<strong>测试数据</strong>， <code>f(x)</code>  应尽量趋近 <code>y</code></p>
<p>最常用的是通过<strong>均方误差最小化</strong>来求 <code>w</code>  和 <code>b</code> ，也称<strong>欧几里得距离</strong>，是 L0 范数</p>
\DeclareMathOperator*{\argmin}{argmin}
(w^*,b^*)=\argmin\limits_{(w,b)}\sum_{i=1}^{m}(f(x_i)-y_i)^2=\argmin\limits_{(w,b)}\sum_{i=1}^{m}(y_i-wx_i-b)^2\tag{1.3}

<p>公式 (1.3) 的意思是：f (xi) 和 yi 均方差取最小时 (w,b) 的集合</p>
<blockquote>
<p>基于均方误差最小化进行模型求解的方法成为<strong>最小二乘法</strong></p>
<p>几何意义：找一条直线使所有样本到该直线的欧氏距离之和最小。如物理实验的描点画直线</p>
</blockquote>
<p>对公式 (1.3) 求最小化时自变量 <code>w</code>  和 <code>b</code>  的取值，称为线性回归的最小二乘<strong>参数估计</strong>。</p>
<p>分别对 <code>w</code>  和 <code>b</code>  求偏导，令偏导为 0 即可求出 <code>w</code>  和 <code>b</code>  的取值</p>
<h4 id="112-多元线性回归"><a class="anchor" href="#112-多元线性回归">#</a> 1.1.2 多元线性回归</h4>
<p>当每条样本有 n 条属性时， <code>w</code>  和 <code>x</code>  是矩阵形式， <code>y</code>  也是向量形式</p>
<blockquote>
<p><strong>X</strong> 是数据集，最后一个元素恒置 1；<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/20170817104340827" alt="这里写图片描述"></p>
<p><strong>y</strong> 是样本标记<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210610202842085.png" alt="image-20210610202842085"></p>
</blockquote>
<p>预测模型即为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>f</mi><mo stretchy="false">(</mo><mi><msub><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">i</mi></msub></mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi mathvariant="bold-italic">w</mi><mi>T</mi></msup><mi><msub><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">i</mi></msub></mi><mo>+</mo><mi>b</mi><mo separator="true">,</mo><mtext>使得</mtext><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>≃</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1.4)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">f(\boldsymbol{x_i})=\boldsymbol{w}^T\boldsymbol{x_i}+b,使得f(x_i)\simeq y_i\tag{1.4}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33528199999999997em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0413309999999998em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33528199999999997em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">使</span><span class="mord cjk_fallback">得</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≃</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span><span class="mord">.</span><span class="mord">4</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>为方便讨论将<strong> ω</strong> 和<strong> b</strong> 写在一起，如下。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mover accent="true"><mi>w</mi><mo stretchy="true">^</mo></mover><mo>=</mo><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">;</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1.5)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\widehat{w}=(w;b)\tag{1.5}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.67056em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.67056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span class="svg-align" style="width:calc(100% - 0.16668em);margin-left:0.16668em;top:-3.43056em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewbox="0 0 1062 239" preserveaspectratio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"/></svg></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>此处省略推导过程，利用<strong>最小二乘法</strong>对<strong> ω</strong> 和<strong> b</strong> 进行估计，得出</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mi><mrow><msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo stretchy="true">^</mo></mover><mo mathvariant="bold-italic">∗</mo></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">T</mi></msup><mi mathvariant="bold-italic">X</mi><msup><mo stretchy="false">)</mo><mrow><mo mathvariant="bold-italic">−</mo><mn mathvariant="bold">1</mn></mrow></msup><msup><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">T</mi></msup><mi mathvariant="bold-italic">y</mi></mrow></mi></mtd><mtd width="50%"></mtd><mtd><mtext>(1.6)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\boldsymbol{\widehat{w}^*=(X^TX)^{-1}X^Ty}\tag{1.6}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1432769999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6844399999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="svg-align" style="top:-3.44444em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewbox="0 0 1062 239" preserveaspectratio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"/></svg></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7435539999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mathbf mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel mathbf">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen mathbf">(</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07778em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8932769999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.15972em;">T</span></span></span></span></span></span></span></span><span class="mord boldsymbol" style="margin-right:0.07778em;">X</span><span class="mclose"><span class="mclose mathbf">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">−</span><span class="mord mathbf mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07778em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8932769999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.15972em;">T</span></span></span></span></span></span></span></span><span class="mord boldsymbol" style="margin-right:0.03704em;">y</span></span></span></span><span class="tag"><span class="strut" style="height:1.1432769999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>那么线性回归模型即为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo stretchy="true">^</mo></mover><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo stretchy="true">^</mo></mover><mi>i</mi><mi>T</mi></msubsup><mi><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">T</mi></msup><mi mathvariant="bold-italic">X</mi><msup><mo stretchy="false">)</mo><mrow><mo mathvariant="bold-italic">−</mo><mn mathvariant="bold">1</mn></mrow></msup><msup><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">T</mi></msup><mi mathvariant="bold-italic">y</mi></mrow></mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1.7)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">f(\widehat{\boldsymbol{x}}_i)=\widehat{\boldsymbol{x}}_{i}^{T}\boldsymbol{(X^TX)^{-1}X^Ty}\tag{1.7}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6844399999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span><span class="svg-align" style="top:-3.44444em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewbox="0 0 1062 239" preserveaspectratio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"/></svg></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.165671em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6844399999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span><span class="svg-align" style="top:-3.44444em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewbox="0 0 1062 239" preserveaspectratio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"/></svg></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9156709999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.13734em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mopen mathbf">(</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07778em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8932769999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.15972em;">T</span></span></span></span></span></span></span></span><span class="mord boldsymbol" style="margin-right:0.07778em;">X</span><span class="mclose"><span class="mclose mathbf">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">−</span><span class="mord mathbf mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07778em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8932769999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord boldsymbol mtight" style="margin-right:0.15972em;">T</span></span></span></span></span></span></span></span><span class="mord boldsymbol" style="margin-right:0.03704em;">y</span></span></span></span><span class="tag"><span class="strut" style="height:1.165671em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span><span class="mord">.</span><span class="mord">7</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>然而<strong> X</strong> 常常<strong>不满秩</strong>，如变量数超过样本数。此时可解出多个解，<strong>均能使均方误差最小化</strong>。此时选择哪个解由算法偏好决定，常见的是引入<strong>正则化</strong></p>
<p><strong>总结</strong>：</p>
<p>​		就是通过一顿操作求出了样本每个特征的<strong>权重</strong>和<strong>偏移</strong>（<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210610211038636.png" alt="image-20210610211038636">)，然后通过它预测<strong>新样本</strong>。线性回归的<strong>学习器</strong>就是<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210610211038636.png" alt="image-20210610211038636"></p>
<p>​		类似于物理实验描点画直线，最终得到<strong>学习模型（学习器）</strong>。只不过这里是多元的直线<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/u=3884380577,2563057688&amp;fm=15&amp;gp=0.jpg" alt="img"></p>
<h3 id="12-python-代码实现"><a class="anchor" href="#12-python代码实现">#</a> 1.2 Python 代码实现</h3>
<blockquote>
<p>Python 有装门用于机器学习的 sklearn 库，可直接利用</p>
</blockquote>
<pre class=" language-language-python"><code class="language-language-python">#1. 读取数据
dataset = pd.read_csv('F:\project\watermelon\melon_data1.csv')
#2. 定义样本属性和训练目标，此例是用密度预测含糖率
X = dataset[['密度']] #其中属性值是二维向量：n个样本;n个属性/样本
Y = dataset['含糖率']
#3. 切分训练集和验证集
X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size=0.5,random_state=0)
#4. 建立模型
log_model = LinearRegression()
#5. 训练
log_model.fit(X_train,Y_train)
#6. 预测
Y_pred = log_model.predict(X_test)
#7. 评估（此处用均方差）
print(np.mean(Y_pred-Y_test)**2)
</code></pre>
<p><strong>关于 sklearn.linear_model.LinearRegression</strong></p>
<p>已知线性模型是</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi mathvariant="bold-italic">w</mi><mi>T</mi></msup><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9746609999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p>
<p>其中：</p>
<p>​	- <code>w</code>  是权重，表示特征的重要程度。一维向量</p>
<p>​	- <code>b</code>  是偏置，模型的偏移量。单个数值</p>
<p>​	- <code>x</code>  是样本，一组特征值。一维向量</p>
<pre class=" language-language-python"><code class="language-language-python">LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)
    """
   	创建线性回归模型，以下参数均为可选，一般来说都选默认值
    :param fit_intercept: bool, default=True.是否计算此模型的截距b。如果设置为False，则在计算中不使用截距（即数据应居中）。
    :param normalize: bool, default=False。如果为真，回归系数X将在回归前通过减去平均值并除以l2范数进行归一化。
    :param copy_X: bool, default=True。如果为True，则复制X；否则，它可能会被覆盖。
    :param n_jobs: int, default=None。用于计算的作业数。
    :param positive: bool, default=False。设置为True时，强制系数为正。只有密集阵列才支持此选项。
    :return: sklearn.linear_model._logistic.LogisticRegression
    """
</code></pre>
<p>函数 <code>LinearRegression()</code>  返回一个 <code>LogisticRegression</code>  对象。</p>
<p><strong>常用属性（成员变量）：</strong></p>
<ul>
<li><code>coef_</code> ：<strong>array</strong>。（系数），指的是权重 <code>w</code></li>
<li><code>intercept_ </code> ：<strong>float</strong>。模型的偏置 <code>b</code></li>
</ul>
<p><strong>常用函数：</strong></p>
<ul>
<li><code>fit(X, y, sample_weight=None)</code> ：拟合线性模型（以最小二乘法，即求最小均方差）。
<ul>
<li><code>X</code> ：训练样本特征值。二维数组（行 - 样本，列 - 特征）</li>
<li><code>y</code> ：靶值。一维数组，样本的结果值</li>
<li><code>sample_weight</code> ：每个样品的单独重量（暂时不清楚，可能是属性的权重 2？）</li>
<li>**return：**self。返回拟合后的线性模型，这种不需要去接收，属于修改型的函数</li>
</ul>
</li>
<li><code>predict(X)</code> ：预测
<ul>
<li><code>X</code> ：用于预测的样本</li>
<li>**return：**array。样本的预测值</li>
</ul>
</li>
<li><code>get_params(deep=True)</code> ：以字典形式返回模型参数，包括但不限于 <code>fit_intercept</code>
<ul>
<li><code>deep</code> ：如果为 True，则将返回此估计器的参数以及作为估计器的包含子对象。</li>
<li>**return：**dict</li>
</ul>
</li>
<li><code>set_params(**params)</code> ：设置参数
<ul>
<li><code>**params</code> ：dict</li>
</ul>
</li>
</ul>
<h2 id="2-对数几率回归逻辑回归"><a class="anchor" href="#2-对数几率回归逻辑回归">#</a> 2. 对数几率回归（逻辑回归）</h2>
<p>虽然叫作<strong>回归</strong>，但逻辑回归是<strong>分类器</strong></p>
<h3 id="21-与线性回归的区别"><a class="anchor" href="#21-与线性回归的区别">#</a> 2.1 与线性回归的区别</h3>
<blockquote>
<p>逻辑回归和线性回归一样，都是求 ω 和 b。即学习器均是 β=(ω:b)</p>
</blockquote>
<p><strong>liner 回归：</strong></p>
<pre><code>1. 主要学习**线性模型**。
2. 预测一般是预测该样本在线性模型上的取值（当然取值也可以看做分类）
</code></pre>
<p><strong>logic 回归：</strong></p>
<ol>
<li>通过学习后的<strong>模型</strong>对新样本进行<strong>分类</strong>。</li>
<li>主要通过 ** 跃迁函数（sigmod）** 加强分类效果。</li>
</ol>
<h3 id="22-sigmod-函数"><a class="anchor" href="#22-sigmod函数">#</a> 2.2 sigmod 函数</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621165221963.png" alt="image-20210621165221963"><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621170226764.png" alt="image-20210621170226764"></p>
<p>意义：大于 0 判正，大于 0 判负</p>
<blockquote>
<p>因为单位阶跃函数不连续，所以用 sigmod 函数（对数几率函数 logistic function）替代，因此称为对数几率回归 / 逻辑回归</p>
</blockquote>
<h3 id="23-算法原理"><a class="anchor" href="#23-算法原理">#</a> 2.3 算法原理</h3>
<p>与线性回归稍有不同，这是用于<strong>分类</strong>学习的算法，目标值是离散的</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621163134066.png" alt="image-20210621163134066"></p>
<p>​    其公式是线性回归和跃迁函数的结合</p>
<ol>
<li>
<p>线性函数<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621171121845.png" alt="image-20210621171121845"> 与<em> sigmod</em> 函数<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621165221963.png" alt="image-20210621165221963"> 结合→<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621163134066.png" alt="image-20210621163134066"></p>
</li>
<li>
<p>推出<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621171318629.png" alt="image-20210621171318629">，y 以 0.5 为临界点</p>
</li>
<li>
<p>得出极大似然函数<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621171854071.png" alt="image-20210621171854071">。求出其取<strong>最小值</strong>时<em> β</em> 的取值即为模型解参数</p>
<blockquote>
<p>其中，<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621172030205.png" alt="image-20210621172030205"></p>
</blockquote>
</li>
</ol>
<p>可以<strong>梯度下降法</strong>或<strong>牛顿法</strong>解</p>
<p>其中关于 β 的一阶、二阶导数分别为<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621172247380.png" alt="image-20210621172247380"> 可用来求梯度<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621172300230.png" alt="image-20210621172300230"></p>
<p>** 总结：**logic 回归就是求模型<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621163134066.png" alt="image-20210621163134066"> 的参数 ω 和 b。</p>
<h3 id="24-代码"><a class="anchor" href="#24-代码">#</a> 2.4 代码</h3>
<pre class=" language-language-python"><code class="language-language-python">#1. 读取数据
dataset = pd.read_csv('F:\project\watermelon\melon_data1.csv')
#2. 定义样本属性和训练目标，此例是用密度预测含糖率
X = dataset.loc[:,['密度','含糖率']] #其中属性值是二维向量：n个样本;n个属性/样本
Y = dataset['含糖率']
#3. 切分训练集和验证集
X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size=0.5,random_state=0)
#4. 建立模型
log_model = LogisticRegression() #逻辑回归，就是对数线性回归，离散分类
#5. 训练
log_model.fit(X_train,Y_train)
#6. 预测
Y_pred = log_model.predict(X_test)
#7. 评估（此处用精确度，即预测到到的/没预测到的）
1-sum(Y_pred-Y_test)/len(Y_test)
</code></pre>
<p><strong>关于 sklearn.linear_model.LogisticRegression</strong></p>
<p>已知逻辑回归模型<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621163134066.png" alt="image-20210621163134066"></p>
<p>其中：</p>
<p>​	- <code>w</code>  是权重，表示特征的重要程度。一维向量</p>
<p>​	- <code>b</code>  是偏置，模型的偏移量。单个数值</p>
<p>​	- <code>x</code>  是样本，一组特征值。一维向量</p>
<pre class=" language-language-python"><code class="language-language-python">LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)
#虽然有这么多的参数，但一般默认就行
log_model = LogisticRegression()#直接这样就行
</code></pre>
<p>函数 <code>LogisticRegression()</code>  返回一个 <code>LogisticRegression</code>  对象。</p>
<p><strong>常用属性（成员变量）：</strong></p>
<ul>
<li><code>coef_</code> ：<strong>array</strong>。（系数），指的是权重 <code>w</code></li>
<li><code>intercept_ </code> ：<strong>float</strong>。模型的偏置 <code>b</code></li>
<li><code>classes_ </code> ：<strong>ndarray</strong>。分类器已知的类标签列表（逻辑回归可以是多分类）</li>
<li><code>n_iter_</code> ：<strong>ndarray</strong> 。各分类的实际<strong>迭代次数</strong>。</li>
</ul>
<p><strong>常用函数：</strong></p>
<ul>
<li><code>fit(X, y, sample_weight=None)</code> ：拟合模型。</li>
<li><code>predict(X)</code> ：预测</li>
<li><code>get_params(deep=True)</code> ：以字典形式返回模型参数，包括但不限于 <code>fit_intercept</code></li>
<li><code>set_params(**params)</code> ：设置参数</li>
<li><code>decision_function(X)</code> ：预测样本的置信度。样本的置信度与样本到超平面的有符号距离成正比。</li>
</ul>
<h1 id="四-决策树"><a class="anchor" href="#四-决策树">#</a> 四、 决策树</h1>
<p>决策树主要就是靠计算<strong>信息增益</strong>或<strong>基尼指数</strong>决定选哪条支线，属于<strong>分类</strong>算法。</p>
<h1 id="五-神经网络初阶"><a class="anchor" href="#五-神经网络初阶">#</a> 五、 神经网络初阶</h1>
<blockquote>
<p>神经网络这块西瓜书讲得不是很详细，而且神经网络本来就是不小于机器学习的模块，所以我这里主要参考了别的一些专门讲<strong>深度学习</strong>的书。</p>
</blockquote>
<p>参考：《深度学习入门<em>基于 python 的理论与实现</em>》</p>
<p>神经网络是个层次递进的结构，每层通过一定的函数计算到下一层，如图所示</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/neural_network.jpg" alt="neural_network"></p>
<p>在此之前，先来熟悉一下神经网络的一些概念</p>
<ul>
<li><strong>权重 ω：<strong>代表各参数的</strong>重要性</strong></li>
<li><strong>偏置 b：<strong>在神经网络中表示该神经元被激活的</strong>容易程度</strong>；</li>
<li>** 激活函数：** 神经网络中，将每层输入信号 (a=wx+b) 转换为输出信号，其作用是决定该神经元是否被激活</li>
<li><strong>张量 (tensor)：<strong>神经网络使用的数据存储在</strong>多维 Numpy 数组</strong>中，也叫<strong>张量（tensor）</strong>。&lt;font color=red&gt; 所以张量其实就是多维数组 &lt;/font&gt;，之所以不能叫做矩阵，矩阵只是二维的数组，张量所指的维度是没有限制的。一般来说，当前所有机器学习系统都使用张量作为基本数据结构。张量这一概念的核心在于，它是一个<strong>数据容器</strong>。它包含的数据几乎总是<strong>数值数据</strong>，因此它是数字的容器。<strong>矩阵</strong>就是<strong>二维张量</strong>。张量是矩阵向<strong>任意维度</strong>的扩展。</li>
<li><strong>梯度 (grad)：<strong>由</strong>全部</strong>变量的<strong>偏导数</strong>汇总而成的<strong>张量</strong>称为<strong>梯度 (gradient)</strong>。如 grad_xy=(dz/dx,dz/dy)</li>
</ul>
<h2 id="1-激活函数"><a class="anchor" href="#1-激活函数">#</a> 1. 激活函数</h2>
<p>有下激活函数所示，绝大多数激活函数在特定范围为 0，即不被激活，也就意味着该神经元对后序网络没有信息传递</p>
<h3 id="11-阶跃函数"><a class="anchor" href="#11-阶跃函数">#</a> 1.1 阶跃函数</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220755046.png" alt="image-20210621220755046"></p>
<h3 id="12-sigmoid-函数"><a class="anchor" href="#12-sigmoid函数">#</a> 1.2 sigmoid 函数</h3>
<p>​     <img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220802439.png" alt="image-20210621220802439"></p>
<h3 id="13-relu-函数"><a class="anchor" href="#13-relu函数">#</a> 1.3 ReLU 函数</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220811079.png" alt="image-20210621220811079"></p>
<h3 id="14-softmax-函数最后一层输出层的激活函数"><a class="anchor" href="#14-softmax函数最后一层输出层的激活函数">#</a> 1.4 softmax 函数（最后一层输出层的激活函数）</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220818680.png" alt="image-20210621220818680"></p>
<h3 id="15-函数图"><a class="anchor" href="#15-函数图">#</a> 1.5 函数图</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220826903.png" alt="image-20210621220826903"></p>
<p>其中：深蓝色的是 relu，黑色虚线是阶跃函数，蓝绿色的是 sigmod 函数</p>
<h2 id="2-神经网络的层次递进"><a class="anchor" href="#2-神经网络的层次递进">#</a> 2. 神经网络的层次递进</h2>
<p>如上图：<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/neural_network.jpg" alt="neural_network"></p>
<p>其中<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621221751826.png" alt="image-20210621221751826"></p>
<blockquote>
<p><strong>X</strong>：特征矩阵。<strong>ω</strong>：各属性权重的矩阵。</p>
</blockquote>
<p>注意以下几点：</p>
<ol>
<li>
<p><strong>输出层</strong>神经元一般取决于<strong>分类数</strong>（几分类就几个），<strong>输入</strong>神经元一般取决于<strong>属性 / 特征</strong>个数</p>
</li>
<li>
<p><strong>x-&gt;a (隐藏层)-&gt; 输出层</strong>是<strong>矩阵运算</strong></p>
</li>
<li>
<p><code>x</code>  是输入 <code>1*n</code>  的向量， <code>n</code>  代表<strong>特征值数量</strong></p>
</li>
</ol>
<p>​       <code>ω</code>  是 <code>n*m</code>  的权值，行：n 个特征值，列：下一层 m 个神经元</p>
<p>​       <code>b</code>  是 <code>1*m</code>  的向量（b 的每个元素并不一样）</p>
<ol start="4">
<li>
<p>** 激活函数 <code>h()</code> ** 用于隐藏层将输入→输出，及 <code>a()→z()</code></p>
</li>
<li>
<p>输出层的激活函数<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621222800025.png" alt="image-20210621222800025"> 要视情况而定，</p>
<p>一般而言：</p>
<ul>
<li>回归（连续）：恒等函数（输入即输出）</li>
<li>分类：</li>
</ul>
<p>​       二元分类： <code>sigmoid()</code> <img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621220802439.png" alt="image-20210621220802439"></p>
<p>​       多分类： <code>softmax()</code> <img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621223227999.png" alt="image-20210621223227999"></p>
</li>
</ol>
<h2 id="3-神经网络的学习"><a class="anchor" href="#3-神经网络的学习">#</a> 3. 神经网络的学习</h2>
<blockquote>
<p>模型的学习主要是为了找到合适的<strong>权重 <code>ω</code> <strong> 和</strong>偏置 <code>b</code> </strong>。学习的过程就是 <code>ω</code>  和 <code>b</code> <strong> 更新</strong>的过程</p>
</blockquote>
<p>大致学习过程如下：</p>
<ol>
<li>** 初始化：** 选取初始权重 <code>ω</code>  和偏置 <code>b</code></li>
<li>** 预测：** 根据损失函数求误差</li>
<li><strong>损失函数求梯度：<strong>主要是求关于</strong>权重</strong>的梯度。一般用求导公式或<strong>误差反向传播算法</strong></li>
<li><strong>更新权重：<strong>向损失函数</strong>梯度反方向</strong>更新</li>
<li>** 预测：** 是否满足要求？是→结束；否→继续 3,4,5</li>
</ol>
<h3 id="31-特征提取与算法设计"><a class="anchor" href="#31-特征提取与算法设计">#</a> 3.1 特征提取与算法设计</h3>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210621223607967.png" alt="image-20210621223607967"></p>
<p>神经网络是没有人为介入，自动提取特征值的。如直接提取图像的本质数据（像素什么的）</p>
<h3 id="32-损失函数"><a class="anchor" href="#32-损失函数">#</a> 3.2 损失函数</h3>
<ul>
<li>
<p>用于表示神经网络<strong>优劣</strong>的指标，损失<strong>越小</strong>，神经网络性能越好。类似误差率。</p>
</li>
<li>
<p>通过不断更新<strong>权重 ω</strong> 来使损失最小化→以使神经网络最优</p>
<ul>
<li>一般使用<strong>梯度下降法</strong>，沿损失函数<strong>梯度反方向</strong>（损失函数减小的方向）更新权重</li>
<li><strong>梯度</strong>可以直接求导公式求，也可以用<strong>误差反向传播法</strong>求梯度</li>
</ul>
</li>
<li>
<p>一般用的比较多的损失函数是<strong>均方误差</strong>和<strong>交叉熵误差</strong></p>
</li>
</ul>
<h4 id="321-均方误差"><a class="anchor" href="#321-均方误差">#</a> 3.2.1 均方误差</h4>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210622160555811.png" alt="image-20210622160555811"></p>
<p><code>yk</code>  和 <code>tk</code>  分别是预测值和实际值，y 是长度为 n 的一维向量</p>
<pre class=" language-language-python"><code class="language-language-python">def mean_squared_error(y,t):
    """
    损失函数：均方误差
    :param y: 预测值,np数组
    :param t: 监督值,np数组
    :return: 均方误差float
    """
    return 0.5*np.sum((y-t)**2)
</code></pre>
<h4 id="322-交叉熵误差"><a class="anchor" href="#322-交叉熵误差">#</a> 3.2.2 交叉熵误差</h4>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210622160932219.png" alt="image-20210622160932219"></p>
<pre class=" language-language-python"><code class="language-language-python">def cross_entropy_error(y,t):
    """
    损失函数：交叉熵误差
    :param y: 预测值，np数组
    :param t: 监督值,np数组
    :return: 交叉熵误差float
    """
    delta=1e-7  #10的-7次，为了防止log0导致的下溢
    return -np.sum(t*np.log(y+delta))
</code></pre>
<h3 id="33-神经网络学习"><a class="anchor" href="#33-神经网络学习">#</a> 3.3 神经网络学习</h3>
<p><strong>步骤：</strong></p>
<ol>
<li>
<p>抽取 mini-batch</p>
</li>
<li>
<p>计算梯度（权重的梯度）。梯度可以说是参数的参数，用于更新参数的参数</p>
</li>
<li>
<p>更新参数（权重，沿梯度反方向更新）</p>
</li>
<li>
<p>重复 123</p>
</li>
</ol>
<p><strong>代码：</strong></p>
<ol>
<li>
<p>神经网络类</p>
<ol>
<li>初始化权重</li>
<li>预测函数</li>
<li>损失函数</li>
<li>精确率函数</li>
<li>梯度下降函数</li>
</ol>
</li>
<li>
<p>训练学习类</p>
<ol>
<li>MNIST 上导入训练集和数据集</li>
<li>设定学习参数（训练次数、神经网络各层神经元数、小批量样本数、学习率等）</li>
<li>迭代计算，各权重沿自己梯度反方向更新，并保存精度变化</li>
<li>绘制图形</li>
</ol>
</li>
</ol>
<h3 id="34-误差反向传播求梯度"><a class="anchor" href="#34-误差反向传播求梯度">#</a> 3.4 误差反向传播求梯度</h3>
<p>通过反向传播求得损失函数的梯度（各自变量的导数），然后即可沿梯度反方向<strong>更新权重</strong>。</p>
<p>反向传播可以比较<strong>快</strong>地计算出梯度。</p>
<blockquote>
<p>原理：局部求导</p>
<p>公式：输出梯度 = 输入梯度 * 本层导数</p>
</blockquote>
<p>实际上和复合函数求导是一样的道理，图中：上面的是价格，下面的是梯度</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/3.jpg" alt="neural_network"></p>
<h4 id="341-关于-affine-层的矩阵反向求导"><a class="anchor" href="#341-关于affine层的矩阵反向求导">#</a> 3.4.1 关于 Affine 层的矩阵反向求导</h4>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210622162354257.png" alt="image-20210622162354257"></p>
<blockquote>
<p>其中，</p>
<p>​    X：特征值向量（N*m）；</p>
<p>​    W：权重矩阵（m*n）；</p>
<p>​    B：偏移向量（1*n）；</p>
<p>​    Y：输出数组（N*n）</p>
<p>​		N 为批量数，因为小批量处理可以输入 N 组向量输出 N 组向量，通过矩阵乘法</p>
</blockquote>
<p>反向传播：</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210622165007115.png" alt="image-20210622165007115"></p>
<blockquote>
<p>L 是损失函数，Y 作为 loss 函数的自变量，<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210622165455298.png" alt="image-20210622165455298"> 是上游传来的导数</p>
<p>对于<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210726173859452.png" alt="image-20210726173859452">，W 相当于是常数；同理，对于<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/image-20210726173914864.png" alt="image-20210726173914864">，X 是常数</p>
</blockquote>
<p>&lt;font color=red&gt;Affine 层各权重梯度求出后实时保存 &lt;/font&gt;</p>
<p>代码：</p>
<pre class=" language-language-python"><code class="language-language-python">class Affine:
    def __init__(self, W, b):
        self.W =W
        self.b = b
        
        self.x = None
        self.original_x_shape = None
        # 权重和偏置参数的导数
        self.dW = None
        self.db = None

    def forward(self, x):
        # 对应张量（相当于维度，矩阵是二维张量）
        self.original_x_shape = x.shape
        x = x.reshape(x.shape[0], -1)
        self.x = x
        out = np.dot(self.x, self.W) + self.b
        return out
    
	#反向传播
    def backward(self, dout):
        """
        dout是上游传下来的梯度
        """
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        # Affine层各权重梯度求出后实时保存
        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）
        return dx
</code></pre>
<h4 id="342-最上游导数生成softmax_with_loss损失函数相对于-softmax-的输出-y-的伪梯度矩阵"><a class="anchor" href="#342-最上游导数生成softmax_with_loss损失函数相对于softmax的输出y的伪梯度矩阵">#</a> 3.4.2 最上游导数生成，softmax_with_loss，损失函数相对于 softmax 的输出 y 的伪梯度矩阵</h4>
<pre class=" language-language-python"><code class="language-language-python">class SoftmaxWithLoss:
    """
    softmax层和loss函数层一起
    其实就是y-l的关系，即最上游的导数求解
    """
    def __init__(self):
        self.loss = None
        self.y = None # softmax的输出
        self.t = None # 监督数据

    def forward(self, x, t):
        """
        得到softmax输出和误差函数输出，还有监督数据
        :param x: 
        :param t: 监督值,np数组
        :return: 交叉熵误差
        """
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        
        return self.loss

    def backward(self, dout=1):
        """
        求dl/dy
        :param dout:
        :return: (y1-t1,y2-t2,y3-t3...)差分数组
        loss中相对于y的梯度，差分数组代表梯度数组，越接近0越精确。各元素往梯度负方向更新即可
        t和y都是多组一维向量组成的二维向量
        """
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况，one-hot-vector就是[0,0,0,0,1,0,0,0,0]表示4
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            #np.arange是0,1,2,3,4,....
            #t是[]表示的是每个样本（每组特征向量）的监督值
            #这行的意思是将dx对应行（一行代表一个样本的预测值概率向量）的对应位置（位置是t的监督值，比如监督值为4，对应dx中第5个元素的概率位置）的元素-1
            dx = dx / batch_size
        
        return dx
</code></pre>
<h4 id="343-反向传播求梯度代码全貌"><a class="anchor" href="#343-反向传播求梯度代码全貌">#</a> 3.4.3 反向传播求梯度代码全貌</h4>
<pre class=" language-language-python"><code class="language-language-python">def gradient(self, x, t):
    #为什么只要导入x呢，因为自变量W和b都在params中保存着了
    W1, W2 = self.params['W1'], self.params['W2']
    b1, b2 = self.params['b1'], self.params['b2']
    grads = &#123;&#125;

    batch_num = x.shape[0]

    # forward
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    y = softmax(a2)

    # backward
    dy = (y - t) / batch_num #差分数组，dy是最上层微分（梯度），是公式中的dl/dy
    grads['W2'] = np.dot(z1.T, dy)#公式中的dl/dw
    grads['b2'] = np.sum(dy, axis=0)
    #Affine层反向传播的时候沿途就保存权重的梯度了

    da1 = np.dot(dy, W2.T)#X*W+B=Y，a是此式中的X

    dz1 = sigmoid_grad(a1) * da1#sigmoid层反向传播（sigmoid层是直接求导的）
    grads['W1'] = np.dot(x.T, dz1)
    grads['b1'] = np.sum(dz1, axis=0)

    return grads
</code></pre>
<h2 id="4-神经网络的代码设计"><a class="anchor" href="#4-神经网络的代码设计">#</a> 4. 神经网络的代码设计</h2>
<blockquote>
<p>y - 预测值；t - 标签值</p>
<p>x - 样本；w - 权重；b - 偏置</p>
</blockquote>
<h3 id="41-functionpy"><a class="anchor" href="#41-functionpy">#</a> 4.1 <span class="exturl" data-url="aHR0cDovL2Z1bmN0aW9uLnB5">function.py</span></h3>
<p>包括各激活函数的、损失函数的实现以及反向传播的局部梯度</p>
<h4 id="411-正向传播"><a class="anchor" href="#411-正向传播">#</a> 4.1.1 正向传播</h4>
<p>sigmoid 函数：</p>
<pre class=" language-language-python"><code class="language-language-python">def sigmoid(x):
    return 1 / (1 + np.exp(-x))
</code></pre>
<p>ReLU 函数：</p>
<pre class=" language-language-python"><code class="language-language-python">def relu(x):
    return np.maximum(0, x)
softmax函数：  
 
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # 溢出对策
    return np.exp(x) / np.sum(np.exp(x))
</code></pre>
<h4 id="412-反向传播"><a class="anchor" href="#412-反向传播">#</a> 4.1.2 反向传播</h4>
<p>反向传播本质上是局部求导</p>
<pre class=" language-language-python"><code class="language-language-python">def sigmoid_grad(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)
def relu_grad(x):
    grad = np.zeros(x)
    grad[x>=0] = 1
    return grad
</code></pre>
<h4 id="413-损失函数"><a class="anchor" href="#413-损失函数">#</a> 4.1.3 损失函数</h4>
<p>损失函数在最后 softmax 处理后算就可以了</p>
<p>均方误差：</p>
<pre class=" language-language-python"><code class="language-language-python">def mean_squared_error(y, t): #均方差误差
    return 0.5 * np.sum((y-t)**2)
</code></pre>
<p>交叉熵误差：</p>
<pre class=" language-language-python"><code class="language-language-python">def cross_entropy_error(y, t): #交叉熵误差
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引
    if t.size == y.size:
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size

最终损失函数
def softmax_loss(X, t): #这里选用的是交叉熵误差
    y = softmax(X)
    return cross_entropy_error(y, t)
</code></pre>
<h3 id="42-layerpy-各层的类"><a class="anchor" href="#42-layerpy各层的类">#</a> 4.2 layer.py 各层的类</h3>
<p>相同结构的层建立一个类，包括各<strong>激活函数</strong>、<strong>Affine</strong>、<strong>卷积池化</strong>、以及<strong> softmax</strong> 和<strong> loss</strong></p>
<p>每层包括初始化函数、正向传播、反向传播</p>
<p>​    输入输出均为矩阵形式。正向传播输入输出是参数矩阵，反向输入输出是梯度矩阵</p>
<h4 id="421-relu-层"><a class="anchor" href="#421-relu层">#</a> 4.2.1 ReLU 层</h4>
<pre class=" language-language-python"><code class="language-language-python">class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0) out="x.copy()" out[self.mask]="0" return def backward(self, dout): dout[self.mask]="0" dx="dout" < code></=></code></pre>
<h4 id="422-sigmoid-层"><a class="anchor" href="#422-sigmoid层">#</a> 4.2.2 sigmoid 层</h4>
<pre class=" language-language-python"><code class="language-language-python">class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = sigmoid(x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out

        return dx
</code></pre>
<h4 id="423-affine-层全连接层也就是-awxb-那层"><a class="anchor" href="#423-affine层全连接层也就是awxb那层">#</a> 4.2.3 Affine 层（全连接层，也就是 a=wx+b 那层）</h4>
<pre class=" language-language-python"><code class="language-language-python">class Affine:
    def __init__(self, W, b):
        self.W =W
        self.b = b
        
        self.x = None
        self.original_x_shape = None
        # 权重和偏置参数的导数
        self.dW = None
        self.db = None

    def forward(self, x):
        # 对应张量
        self.original_x_shape = x.shape
        x = x.reshape(x.shape[0], -1)
        self.x = x

        out = np.dot(self.x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        
        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）
        return dx
</code></pre>
<h4 id="424-卷积层"><a class="anchor" href="#424-卷积层">#</a> 4.2.4 卷积层</h4>
<pre class=" language-language-python"><code class="language-language-python">class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        
        # 中间数据（backward时使用）
        self.x = None   
        self.col = None
        self.col_W = None
        
        # 权重和偏置参数的梯度
        self.dW = None
        self.db = None

    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        # print("输入矩阵初始形状")
        # print(x.shape)
        N, C, H, W = x.shape
        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)
        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T
        # print("形状")
        # print(col.shape,col_W.shape)

        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        self.x = x
        self.col = col
        self.col_W = col_W

        return out

    def backward(self, dout):
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(0,2,3,1).reshape(-1, FN)

        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        return dx
</code></pre>
<h4 id="425-池化层"><a class="anchor" href="#425-池化层">#</a> 4.2.5 池化层</h4>
<pre class=" language-language-python"><code class="language-language-python">class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = None
        self.arg_max = None

    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)

        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)

        arg_max = np.argmax(col, axis=1)
        out = np.max(col, axis=1)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        self.x = x
        self.arg_max = arg_max

        return out

    def backward(self, dout):
        dout = dout.transpose(0, 2, 3, 1)
        
        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,)) 
        
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
        
        return dx
</code></pre>
<h4 id="426-softmax-至损失函数层"><a class="anchor" href="#426-softmax至损失函数层">#</a> 4.2.6 softmax 至损失函数层</h4>
<p>将 softmax 与 loss 合并为一层</p>
<pre class=" language-language-python"><code class="language-language-python">class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None # softmax的输出
        self.t = None # 监督数据

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        
        return dx
</code></pre>
<h3 id="43-neuralnet-神经网络类"><a class="anchor" href="#43-neuralnet神经网络类">#</a> 4.3 Neuralnet 神经网络类</h3>
<p>包括初始化函数（初始化各超参数）、预测函数（神经网络层次递进）、损失函数、准确率函数、梯度函数（学习过程中保存每层各权重的梯度）</p>
<p>以卷积神经网络为例</p>
<h4 id="431-类变量"><a class="anchor" href="#431-类变量">#</a> 4.3.1 类变量</h4>
<p>权重参数（字典形式：key - 名称；value - 矩阵）</p>
<pre class=" language-language-python"><code class="language-language-python"># 初始化权重
self.params = &#123;&#125;
self.params['W1'] = weight_init_std * \
                    np.random.randn(filter_num, input_dim[0], filter_size, filter_size)
self.params['b1'] = np.zeros(filter_num)
self.params['W2'] = weight_init_std * \
                    np.random.randn(pool_output_size, hidden_size)
self.params['b2'] = np.zeros(hidden_size)
self.params['W3'] = weight_init_std * \
                    np.random.randn(hidden_size, output_size)
self.params['b3'] = np.zeros(output_size)
</code></pre>
<p>各个层对象（无序表。）</p>
<pre class=" language-language-python"><code class="language-language-python"># 生成层
self.layers = OrderedDict()
self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],
                                   conv_param['stride'], conv_param['pad'])
self.layers['Relu1'] = Relu()
self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
self.layers['Relu2'] = Relu()
self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])

self.last_layer = SoftmaxWithLoss()
</code></pre>
<h4 id="432-初始化函数主要用于定义类变量"><a class="anchor" href="#432-初始化函数主要用于定义类变量">#</a> 4.3.2 初始化函数（主要用于定义类变量）</h4>
<pre class=" language-language-python"><code class="language-language-python">def __init__(self, input_dim=(1, 28, 28), 
             conv_param=&#123;'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1&#125;,
             hidden_size=100, output_size=10, weight_init_std=0.01):
</code></pre>
<p>包括：</p>
<p>输入矩阵的形状大小（特征值的数量和排列）</p>
<p>滤波器的参数（形状、步幅、填充等，滤波器是卷积层的权重）</p>
<p>各层的神经元数量</p>
<p>标准差（初始化权重 = 标准差 * 范围随机数）</p>
<h4 id="433-预测函数"><a class="anchor" href="#433-预测函数">#</a> 4.3.3 预测函数</h4>
<pre class=" language-language-python"><code class="language-language-python">def predict(self, x): 
#层次递进，计算到输出层softmax处理前（这里softmax和loss合并了）
    for layer in self.layers.values():
        x = layer.forward(x)

    return x
</code></pre>
<h4 id="434-损失函数"><a class="anchor" href="#434-损失函数">#</a> 4.3.4 损失函数</h4>
<pre class=" language-language-python"><code class="language-language-python">def loss(self, x, t):
    """求损失函数
    参数x是输入数据、t是教师标签
       就是比predict多往前走一步（进行softmax处理并求交叉熵误差）
    """
    y = self.predict(x)
    return self.last_layer.forward(y, t)
</code></pre>
<h4 id="435-计算精度"><a class="anchor" href="#435-计算精度">#</a> 4.3.5 计算精度</h4>
<pre class=" language-language-python"><code class="language-language-python">def accuracy(self, x, t, batch_size=100):
    if t.ndim != 1 : t = np.argmax(t, axis=1)
    
    acc = 0.0
    
    for i in range(int(x.shape[0] / batch_size)):
        tx = x[i*batch_size:(i+1)*batch_size]
        tt = t[i*batch_size:(i+1)*batch_size]
        y = self.predict(tx)
        y = np.argmax(y, axis=1)
        acc += np.sum(y == tt) 
    
    return acc / x.shape[0]
</code></pre>
<h4 id="436-求梯度"><a class="anchor" href="#436-求梯度">#</a> 4.3.6 求梯度</h4>
<pre class=" language-language-python"><code class="language-language-python">def gradient(self, x, t):
    """求梯度（误差反向传播法）
    Parameters
    ----------
    x : 输入数据
    t : 标签值
    Returns
    -------
    具有各层的梯度的字典变量
        grads['W1']、grads['W2']、...是各层的权重
        grads['b1']、grads['b2']、...是各层的偏置
    """
    # forward
    self.loss(x, t)

    # backward
    dout = 1
    dout = self.last_layer.backward(dout)

    layers = list(self.layers.values())
    layers.reverse()
    for layer in layers:
        dout = layer.backward(dout)

    # 设定
    grads = &#123;&#125;
    grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
    grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
    grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

    return grads
</code></pre>
<h4 id="437-保存参数学习完后保存下最终权重等参数"><a class="anchor" href="#437-保存参数学习完后保存下最终权重等参数">#</a> 4.3.7 保存参数（学习完后保存下最终权重等参数）</h4>
<pre class=" language-language-python"><code class="language-language-python">def save_params(self, file_name="params.pkl"):
    params = &#123;&#125;
    for key, val in self.params.items():
        params[key] = val
    with open(file_name, 'wb') as f:
        pickle.dump(params, f)
</code></pre>
<h4 id="438-加载参数预测时加载之前学习到的参数"><a class="anchor" href="#438-加载参数预测时加载之前学习到的参数">#</a> 4.3.8 加载参数（预测时加载之前学习到的参数）</h4>
<pre class=" language-language-python"><code class="language-language-python">def load_params(self, file_name="params.pkl"):
    with open(file_name, 'rb') as f:
        params = pickle.load(f)
    for key, val in params.items():
        self.params[key] = val

    for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):
        self.layers[key].W = self.params['W' + str(i+1)]
        self.layers[key].b = self.params['b' + str(i+1)]
</code></pre>
<h3 id="44-trainer-训练类学习类"><a class="anchor" href="#44-trainer训练类学习类">#</a> 4.4 Trainer 训练类（学习类）</h3>
<p>包括初始化函数（确认训练测试集以及其他训练所用参数等）、训练函数等</p>
<h4 id="441-类变量及初始化函数"><a class="anchor" href="#441-类变量及初始化函数">#</a> 4.4.1 类变量及初始化函数</h4>
<p>初始化：<br>
训练所用的 训练测试集、分片、epoch、参数更新算法、神经网络、训练次数</p>
<p>初始化函数：主要用于定义、初始化变量</p>
<pre class=" language-language-python"><code class="language-language-python">def __init__(self, network, x_train, t_train, x_test, t_test,
             epochs=20, mini_batch_size=100,
             optimizer='SGD', optimizer_param=&#123;'lr':0.01&#125;, 
             evaluate_sample_num_per_epoch=None, verbose=True):
    self.network = network#神经网络（type=神经网络类）
    self.verbose = verbose#是否实时输出迭代信息
    self.x_train = x_train#训练样本数据集（type=np.array）
    self.t_train = t_train#训练监督数据集
    self.x_test = x_test#测试样本数据集
    self.t_test = t_test#测试监督数据集
    self.epochs = epochs#epoch的数量。epoch，把所有样本都过一遍的分片循环次数
    self.batch_size = mini_batch_size#分片大小
    self.evaluate_sample_num_per_epoch#每个epoch有几个样本数，默认None，也就是所有样本/epoch
# optimzer：优化器（优化算法，更新权重等参数的算法）
optimizer_class_dict = &#123;'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov, 'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam&#125;
    
self.optimizer =   
optimizer_class_dict[optimizer.lower()](**optimizer_param)
#SGD(&#123;'lr':0.01&#125;),**表示参数以字典形式导入
    self.train_size = x_train.shape[0]#训练数据集大小
    self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)#1个epoch的大小，把所有样本都过一遍的分片循环次数
    self.max_iter = int(epochs * self.iter_per_epoch)#迭代次数（训练次数，以学习一个分片为一次）
 
</code></pre>
<p>训练中：<br>
实时保存 当前训练次数、当前精度、当前损失函数</p>
<pre class=" language-language-python"><code class="language-language-python">    self.current_iter = 0#当前迭代次数
    self.current_epoch = 0#当前epoch数
    #记录下迭代信息，方便画图（以epoch为单位）
    self.train_loss_list = []#损失函数
    self.train_acc_list = []#训练准确率
    self.test_acc_list = []#测试准确率
</code></pre>
<h4 id="442-训练函数"><a class="anchor" href="#442-训练函数">#</a> 4.4.2 训练函数</h4>
<p>（每个分片）训练学习的函数，其中包括<br>
分片、计算梯度、按梯度更新权重、计算保存损失函数、计算保存训练测试精度等</p>
<pre class=" language-language-python"><code class="language-language-python">def train_step(self):
    """
    （每个分片）训练学习的函数，其中包括
        分片、计算梯度、按梯度更新权重、计算保存损失函数、计算保存训练测试精度等
    Returns
    -------
    """
    batch_mask = np.random.choice(self.train_size, self.batch_size)#从[0,train_size)中随机选取batch_size个数字
    x_batch = self.x_train[batch_mask]#分片大小<=batch_size，因为batch_mask中有重复元素 t_batch="self.t_train[batch_mask]" #神经网络类的gradient函数封装了实时保存权重梯度的功能 grads="self.network.gradient(x_batch," t_batch) self.optimizer.update(self.network.params, grads)#按照梯度更新权重 #计算保存损失函数 loss="self.network.loss(x_batch," self.train_loss_list.append(loss) if self.verbose: print("train loss:" + str(loss)) self.current_iter % self.iter_per_epoch="=" 0: self.current_epoch #一次epoch结束保存训练测试精度 x_train_sample, t_train_sample="self.x_train," self.t_train x_test_sample, t_test_sample="self.x_test," self.t_test # 指定epoch大小的情况下，一般不指定，默认全部样本为一个epoch not self.evaluate_sample_num_per_epoch is none: t="self.evaluate_sample_num_per_epoch" self.t_train[:t] self.t_test[:t] train_acc="self.network.accuracy(x_train_sample," t_train_sample) test_acc="self.network.accuracy(x_test_sample," t_test_sample) self.train_acc_list.append(train_acc) self.test_acc_list.append(test_acc) print("="==" epoch:" str(self.current_epoch) ", train acc:" str(train_acc) test str(test_acc) "="==")" < code></=batch_size，因为batch_mask中有重复元素></code></pre>
<h4 id="443-最终训练函数提供给外界的-api"><a class="anchor" href="#443-最终训练函数提供给外界的api">#</a> 4.4.3 最终训练函数（提供给外界的 API）</h4>
<pre class=" language-language-python"><code class="language-language-python">def train(self):
    """
    总训练，
    Returns
    -------
    """
    for i in range(self.max_iter):
        self.train_step()

    #最终精度，用学习完的权重参数最后预测一次测试数据集
    test_acc = self.network.accuracy(self.x_test, self.t_test)

    if self.verbose:
        print("=============== Final Test Accuracy ===============")
        print("test acc:" + str(test_acc))
</code></pre>
<h3 id="45-数据集加载-mnistpy"><a class="anchor" href="#45-数据集加载mnistpy">#</a> 4.5 <span class="exturl" data-url="aHR0cDovL3huLS1tbmlzdC1lODZoeDcxaHFtZGhzOWoyaHAucHk=">数据集加载 mnist.py</span></h3>
<h4 id="451-初始化所需参数"><a class="anchor" href="#451-初始化所需参数">#</a> 4.5.1 初始化所需参数</h4>
<ol>
<li>
<p>爬数据的网站 url</p>
</li>
<li>
<p>本地保存路径（网上下载的压缩文件）</p>
</li>
<li>
<p>本地保存路径（用于实验的处理后数据）</p>
</li>
<li>
<p>训练 / 测试次数</p>
</li>
<li>
<p>图片像素 / 一维展开后大小</p>
</li>
</ol>
<pre class=" language-language-python"><code class="language-language-python">url_base = 'http://yann.lecun.com/exdb/mnist/'
key_file = &#123;
    'train_img':'train-images-idx3-ubyte.gz',
    'train_label':'train-labels-idx1-ubyte.gz',
    'test_img':'t10k-images-idx3-ubyte.gz',
    'test_label':'t10k-labels-idx1-ubyte.gz'
&#125;
#父目录路径
dataset_dir = os.path.dirname(os.path.abspath(__file__))
save_file = dataset_dir + "/mnist.pkl"

train_num = 60000
test_num = 10000
img_dim = (1, 28, 28)
img_size = 784
</code></pre>
<h4 id="452-下载所需数据集"><a class="anchor" href="#452-下载所需数据集">#</a> 4.5.2 下载所需数据集</h4>
<ol>
<li>作用：从网站上按照 url+file_name 爬数据，每个文件爬一次即可（第一次执行会有点慢，之后都正常）</li>
<li>下载下来的数据一般需要后序处理（解压缩等）</li>
</ol>
<pre class=" language-language-python"><code class="language-language-python">def _download(file_name):

    file_path = dataset_dir + "/" + file_name
    
    if os.path.exists(file_path):
        return

    print("Downloading " + file_name + " ... ")
 
    # 把url_base+file_name的数据下载到本地file_path
    urllib.request.urlretrieve(url_base + file_name, file_path)
    print("Done")
    
def download_mnist():
    """
    将mnist数据集中需要用到的数据（上面key_file字典中定义的）下载下来
    Returns
    -------

    """
    for v in key_file.values():
       _download(v)
</code></pre>
<h4 id="453-解压打开-gzip-文件"><a class="anchor" href="#453-解压打开gzip文件">#</a> 4.5.3 解压打开 gzip 文件</h4>
<pre class=" language-language-python"><code class="language-language-python">def _load_label(file_name):
    """读取gzip文件并提示，读取成功打印Done
    Returns 文件内容(多维向量)
    """
    file_path = dataset_dir + "/" + file_name
    
    print("Converting " + file_name + " to NumPy Array ...")
    with gzip.open(file_path, 'rb') as f:#打开gzip文件，rb r-只读，b-读取的是二进制文件
            labels = np.frombuffer(f.read(), np.uint8, offset=8)

        # numpy.frombuffer(buffer, dtype=float, count=-1, offset=0)
        # 参数
        #     缓冲区：它表示暴露缓冲区接口的对象。
        #     dtype：代表返回的数据类型数组的数据类型。
        #     count：代表返回的ndarray的长度。默认值为-1。
        #     偏移量：代表读取的起始位置。默认值为0。

    print("Done")

    return labels

def _load_img(file_name):
    """
    打开图像数据的gzip文件并转换成1*784的一维向量
    Parameters
    ----------
    file_name

    Returns 一维展开的输入图像数据（一维向量）
    -------

    """
    file_path = dataset_dir + "/" + file_name
    
    print("Converting " + file_name + " to NumPy Array ...")    
    with gzip.open(file_path, 'rb') as f:
            data = np.frombuffer(f.read(), np.uint8, offset=16)
    data = data.reshape(-1, img_size)
    print("Done")
    
    return data
</code></pre>
<h4 id="454-将数据集信息保存在-dict-中"><a class="anchor" href="#454-将数据集信息保存在dict中">#</a> 4.5.4 将数据集信息保存在 dict 中</h4>
<ol>
<li>主要方便于 init_mnist () 中将数据集保存在本地文件中</li>
</ol>
<pre class=" language-language-python"><code class="language-language-python">def _convert_numpy():
    """
    Returns 以字典dict形式返回训练集/测试集
    key-描述数据集监督数据/测试数据 的字符串
    value-监督数据（label）/测试数据（image）的numpy表示
    -------

    """
    dataset = &#123;&#125;
    dataset['train_img'] =  _load_img(key_file['train_img'])
    dataset['train_label'] = _load_label(key_file['train_label'])    
    dataset['test_img'] = _load_img(key_file['test_img'])
    dataset['test_label'] = _load_label(key_file['test_label'])
    
    return dataset
</code></pre>
<h4 id="455-将解压后的数据集保存到本地"><a class="anchor" href="#455-将解压后的数据集保存到本地">#</a> 4.5.5 将解压后的数据集保存到本地</h4>
<pre class=" language-language-python"><code class="language-language-python">def init_mnist():
    """
    将下载下来的数据保存到本地文件中save_file中
    Returns
    -------

    """
    download_mnist()
    dataset = _convert_numpy()
    print("Creating pickle file ...")
    with open(save_file, 'wb') as f:#wb-写入二进制文件
        pickle.dump(dataset, f, -1)#将dataset的数据保存到f中，本例是save_file文件，即mnist.pkl。-1表示使用最高protocol对dataset压缩
    print("Done!")
</code></pre>
<h4 id="456-监督数据格式转换"><a class="anchor" href="#456-监督数据格式转换">#</a> 4.5.6 监督数据格式转换</h4>
<ol>
<li>如果参数 one_hot_label=False 就不用执行此函数</li>
</ol>
<pre class=" language-language-python"><code class="language-language-python">def _change_one_hot_label(X):
    """
    将监督数据转换成[0,0,1,0,0,0,0,0,0,0]的形式
    Parameters
    ----------
    X 一组监督数据。如[5,9,8,3,4,6]。一维向量

    Returns 一维向量形式的监督数据（一组）
    -------

    """
    T = np.zeros((X.size, 10))#10是因为本例数字识别一共10个数
    for idx, row in enumerate(T):#枚举，同时遍历索引和元素
        #idx-索引，row-（idx索引指向的）元素
        #row是一个监督数据（标签），T是一组
        row[X[idx]] = 1
        
    return T
</code></pre>
<h4 id="457-对外接口-api"><a class="anchor" href="#457-对外接口api">#</a> 4.5.7 对外接口 api</h4>
<pre class=" language-language-python"><code class="language-language-python">def load_mnist(normalize=True, flatten=True, one_hot_label=False):
    """读入MNIST数据集
    
    Parameters
    ----------
    normalize : 将图像的像素值正规化为0.0~1.0
    one_hot_label : 
        one_hot_label为True的情况下，标签作为one-hot数组返回
        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组
    flatten : 是否将图像展开为一维数组
    
    Returns
    -------
    (训练图像, 训练标签), (测试图像, 测试标签)
    """
    if not os.path.exists(save_file):
        init_mnist()
        
    with open(save_file, 'rb') as f:
        dataset = pickle.load(f)#加载本地下载好的数据集
    
    if normalize:
        for key in ('train_img', 'test_img'):
            dataset[key] = dataset[key].astype(np.float32)#转换成固定的数据类型float32
            dataset[key] /= 255.0
            
    if one_hot_label:
        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])
        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])
    
    if not flatten:
         for key in ('train_img', 'test_img'):
            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)

    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) 
</code></pre>
<h3 id="46-执行主函数包括图像显示"><a class="anchor" href="#46-执行主函数包括图像显示">#</a> 4.6 执行主函数（包括图像显示）</h3>
<h4 id="461-读入数据"><a class="anchor" href="#461-读入数据">#</a> 4.6.1 读入数据</h4>
<pre class=" language-language-python"><code class="language-language-python"># 读入数据
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)

# 处理花费时间较长的情况下减少数据 取前5000个样本
#x_train, t_train = x_train[:5000], t_train[:5000]
#x_test, t_test = x_test[:1000], t_test[:1000]

max_epochs = 20#20轮次
</code></pre>
<h4 id="462-创建神经网络对象"><a class="anchor" href="#462-创建神经网络对象">#</a> 4.6.2 创建神经网络对象</h4>
<pre class=" language-language-python"><code class="language-language-python">#建立神经网络
network = SimpleConvNet(input_dim=(1,28,28), 
                        conv_param = &#123;'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1&#125;,
                        hidden_size=100, output_size=10, weight_init_std=0.01)
</code></pre>
<h4 id="463-创建训练对象"><a class="anchor" href="#463-创建训练对象">#</a> 4.6.3 创建训练对象</h4>
<pre class=" language-language-python"><code class="language-language-python">#训练对象，拟定训练参数
trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=max_epochs, mini_batch_size=100,
                  optimizer='Adam', optimizer_param=&#123;'lr': 0.001&#125;,
                  evaluate_sample_num_per_epoch=1000)
</code></pre>
<h4 id="464-训练并保存参数"><a class="anchor" href="#464-训练并保存参数">#</a> 4.6.4 训练并保存参数</h4>
<pre class=" language-language-python"><code class="language-language-python">trainer.train()
# 保存参数
network.save_params("params.pkl")
print("Saved Network Parameters!")
</code></pre>
<h4 id="465-绘图"><a class="anchor" href="#465-绘图">#</a> 4.6.5 绘图</h4>
<pre class=" language-language-python"><code class="language-language-python"># 绘制图形
markers = &#123;'train': 'o', 'test': 's'&#125;
x = np.arange(max_epochs)
plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)
plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
</code></pre>
<h2 id="5-总结"><a class="anchor" href="#5-总结">#</a> 5. 总结</h2>
<ol>
<li>
<p>** 初始化：** 选取初始权重 <code>ω</code>  和偏置 <code>b</code> 。<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM5Nzg5NzcvYXJ0aWNsZS9kZXRhaWxzLzg0ODYxNDUz">参数初始化方法</span></p>
<ol>
<li>包括 <code>reset_parameters</code> 、 <code>.uniform_</code> 等都是相关<strong>初始化</strong>方法</li>
<li><code>torch.empty()</code> 、 <code>torch.rand()</code> 、 <code>torch.Tensor()</code> ：随机生成<strong>张量</strong>的方法</li>
</ol>
</li>
<li>
<p><strong>建立模型：<strong>即神经网络模型。模型包扩但不限于</strong>参数（权重、偏置、梯度...）</strong>、<strong>每层的对象（也算模型）</strong>，神经网络是由多个函数模型组合而成的模型。</p>
<ol>
<li><code>liner()</code> ：线性模型，回归层用到比较多（如果有的话）</li>
<li><code>LSTMCell()</code> ：RNN 的 memory 层</li>
</ol>
</li>
<li>
<p>** 预测：** 根据模型参数求预测值</p>
<ol>
<li><code>forward()</code> ：根据<strong>公式</strong>，将<strong>各层</strong>输入转为输出，在下一层输入。</li>
<li><code>predict()</code> ：主要工作是根据<strong>学习模型</strong>，将<strong>输入</strong>转换成<strong>输出</strong>。是 <code>forward</code>  的集成</li>
</ol>
</li>
<li>
<p><strong>损失函数</strong>：计算预测值和标签值的误差，用于<strong>更新</strong>模型参数</p>
<ol>
<li><code>loss()</code> ：损失函数一般都命名为 loss，主要分<strong>均方差</strong>和<strong>交叉熵</strong> 2 种方法</li>
<li><code>mean_squared_error()</code> ：均方误差</li>
<li><code>cross_entropy_error()</code> ：交叉熵误差</li>
<li><code>binary_cross_entropy_error()</code> ：二元交叉熵</li>
<li><code>binary_cross_entropy_error_with_logits()</code> ：集成 sigmod 的二元交叉熵</li>
</ol>
</li>
<li>
<p><strong>求梯度：<strong>主要是求关于</strong>权重</strong>的梯度。一般用求导公式或<strong>误差反向传播算法</strong></p>
<ol>
<li><code>backward()</code> ：可以说是 <code>forward()</code>  的反函数，根据各层上游的<strong>梯度</strong>及该层公式，计算出此层的<strong>梯度</strong></li>
<li><code>gradient()</code> ：求出<strong>每一层</strong>的梯度，集成了 <code>backward()</code></li>
</ol>
</li>
<li>
<p><strong>更新权重：<strong>向损失函数</strong>梯度反方向</strong>更新参数。(参数包括各层<strong>权重</strong>、<strong>偏置</strong>等，梯度不需要更新，因为梯度是每次都重新求的)</p>
<ol>
<li>
<p><code>params -= learning_rate * grad</code> ：根据<strong>学习率</strong>和<strong>梯度</strong>反向更新即可，这是常规写法</p>
</li>
<li>
<p><code>torch</code>  更新：</p>
<ol>
<li>
<p>新建一个优化器，参数内容：学习率、模型的所有参数（权重、梯度等）</p>
<pre class=" language-language-python"><code class="language-language-python">optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)#此例是SGD优化器，lr：学习率
optim = torch.optim.Adam(model.parameters(), lr = 1e-3)#Adam优化器
</code></pre>
</li>
<li>
<p>通过优化器更新参数</p>
<pre class=" language-language-python"><code class="language-language-python">optim.step() #gradient descent
</code></pre>
<p>&lt;font color=red&gt; 注：用 torch 求梯度时，要先清零，再求梯度，然后下降。如下 &lt;/font&gt;</p>
<pre class=" language-language-python"><code class="language-language-python">optim.zero_grad()#梯度清零，否则会梯度叠加
loss.backward()
optim.step() #gradient descent
</code></pre>
</li>
</ol>
</li>
</ol>
</li>
<li>
<p>** 预测：** 是否满足要求？是→结束；否→继续 3,4,5</p>
</li>
</ol>
<h1 id="五-rnn-循环神经网络"><a class="anchor" href="#五-rnn循环神经网络">#</a> 五 +、 RNN 循环神经网络</h1>
<h2 id="解决梯度爆炸和梯度消失"><a class="anchor" href="#解决梯度爆炸和梯度消失">#</a> 解决梯度爆炸和梯度消失</h2>
<p>LSTM 和 GRU 是用于解决梯度爆炸和梯度消失的变体 RNN，主要思想是增加门控系统，决定哪些太旧影响力太小的参数是否要删除 3</p>
<ul>
<li>梯度消失：太前面的权重经过太久的传播后梯度很难反馈回去，导致靠后的输出基本不受靠前序列的影响。而有时候很久之前的样本数据却刚好非常的重要。</li>
<li>梯度爆炸：梯度变得太大，参数指数级上升导致溢出或模型崩溃，比较容易发现。一般解决方法是梯度修剪</li>
</ul>
<h2 id="2-lstm-长短时记忆"><a class="anchor" href="#2-lstm长短时记忆">#</a> 2. LSTM 长短时记忆</h2>
<blockquote>
<p>LSTM 是为了解决 RNN<strong> 梯度消失</strong>和<strong>梯度爆炸</strong>提出的，可以说是 RNN 的进阶</p>
</blockquote>
<p>简单来说，LSTM 就是通过 <code>cell</code>  在每次迭代中<strong>记住</strong>重要的，<strong>忘记</strong>不重要的，再进入下一次迭代。</p>
<p>相比于 <code>h</code> ， <code>cell</code>  变化幅度更小</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/v2-a022384bc222397009cedd3ce858c8fb_720w.jpg" alt="img"></p>
<h1 id="六-支持向量机"><a class="anchor" href="#六-支持向量机">#</a> 六、 支持向量机</h1>
<p>什么是支持向量机 (SVM)：<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzIxMDk0NDg5JUUzJTgwJTgyJUU1JThGJUFGJUU0JUJCJUE1JUU3JTlDJThCJUU3JTlDJThCJUU4JUJGJTk5JUU0JUI4JUFBJUVGJUJDJThDJUU4JUFFJUIyJUU1JUJFJTk3JUU5JTlEJTlFJUU1JUI4JUI4JUU5JTgwJTlBJUU0JUJGJTk3JUU2JTk4JTkzJUU2JTg3JTgy">https://www.zhihu.com/question/21094489。可以看看这个，讲得非常通俗易懂</span></p>
<blockquote>
<p>大意是找个<strong>函数</strong>尽可能把 2 类东西分开。</p>
</blockquote>
<p>一维：<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/7befaafc45763b9c4469abf245dc98cb_720w.jpg" alt="img"> 二维：<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/55d7ad2a6e23579b17aec0c3c9135eb3_720w.jpg" alt="img"></p>
<h2 id="正则化"><a class="anchor" href="#正则化">#</a> 正则化</h2>
<blockquote>
<ul>
<li>
<p><strong>正则化是为了防止过拟合</strong></p>
</li>
<li>
<p>理论上来讲正则起到的作用就是：“损失精度去调整样本的不足产生的拟合”。</p>
</li>
</ul>
</blockquote>
<p>正则化的英文 Regularizaiton-Regular-Regularize，<strong>直译应该是：规则化</strong></p>
<h1 id="十-knn-算法"><a class="anchor" href="#十-knn算法">#</a> 十、 KNN 算法</h1>
<p>参考：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzkwOTY1NjA=">https://zhuanlan.zhihu.com/p/339096560</span></p>
<h2 id="1-knn-算法"><a class="anchor" href="#1-knn算法">#</a> 1. KNN 算法</h2>
<blockquote>
<p>所谓 K 最近邻，就是 K 个最近的邻居的意思，说的是每个样本都可以用它最接近的 K 个邻近值来代表。</p>
</blockquote>
<ul>
<li>优：简单</li>
<li>缺：计算量大，对每一个待分类的文本都要计算它到全体已知样本的距离。
<ul>
<li>剪辑：事先去除对分类作用不大的样本</li>
</ul>
</li>
</ul>
<h3 id="11-步骤"><a class="anchor" href="#11-步骤">#</a> 1.1 步骤</h3>
<ol>
<li>
<p><strong>距离</strong>度量：常用<strong>欧几里得距离</strong>。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mroot><mrow><munder><mo>∑</mo><mi>i</mi></munder><msup><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub><mo fence="true">∣</mo></mrow><mi>p</mi></msup></mrow><mi>p</mi></mroot></mrow><annotation encoding="application/x-tex">\sqrt[p]{\sum_{i}\left | x_{1}-x_{2} \right |^{p}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.04em;vertical-align:-1.5199570000000002em;"></span><span class="mord sqrt"><span class="root"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.21533159999999973em;"><span style="top:-2.5000516em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.09716840000000025em;"><span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5200429999999998em;"><span class="svg-align" style="top:-5em;"><span class="pstrut" style="height:5em;"></span><span class="mord" style="padding-left:1em;"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-3.2029000000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.4800429999999998em;"><span class="pstrut" style="height:5em;"></span><span class="hide-tail" style="min-width:1.02em;height:3.08em;"><svg width="400em" height="3.08em" viewbox="0 0 400000 3240" preserveaspectratio="xMinYMin slice"><path d="M473,2793
c339.3,-1799.3,509.3,-2700,510,-2702 l0 -0
c3.3,-7.3,9.3,-11,18,-11 H400000v40H1017.7
s-90.5,478,-276.2,1466c-185.7,988,-279.5,1483,-281.5,1485c-2,6,-10,9,-24,9
c-8,0,-12,-0.7,-12,-2c0,-1.3,-5.3,-32,-16,-92c-50.7,-293.3,-119.7,-693.3,-207,-1200
c0,-1.3,-5.3,8.7,-16,30c-10.7,21.3,-21.3,42.7,-32,64s-16,33,-16,33s-26,-26,-26,-26
s76,-153,76,-153s77,-151,77,-151c0.7,0.7,35.7,202,105,604c67.3,400.7,102,602.7,104,
606zM1001 80h400000v40H1017.7z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5199570000000002em;"><span></span></span></span></span></span></span></span></span></span></p>
<ol>
<li>当 p=1 时，就是曼哈顿距离（对应 <code>L1</code>  范数）</li>
<li>当 p=2 时，就是欧氏距离（对应 L2 范数）</li>
<li>当 p→∞时，就是切比雪夫距离</li>
</ol>
</li>
<li>
<p><strong>K 值</strong>选择：距离最近的 K 个样本。通常采用<strong>交叉验证法</strong>来选取最优的 K 值。</p>
<ul>
<li>K 值较小：训练误差↓，测试误差↑。<strong>容易过拟合</strong></li>
<li>K 值较大：训练误差↑，测试误差↓。<strong>容易训练不到位</strong></li>
</ul>
</li>
</ol>
<h3 id="12-代码实现"><a class="anchor" href="#12-代码实现">#</a> 1.2 代码实现</h3>
<p>knn 的库为 <code>KNeighborsRegressor</code></p>
<pre class=" language-language-python"><code class="language-language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
</code></pre>
<p><strong>生成训练数据</strong>：以 sin 函数测试</p>
<pre class=" language-language-python"><code class="language-language-python">np.random.seed(0)
# 随机生成40个(0, 1)之前的数，乘以5，再进行升序
X = np.sort(5 * np.random.rand(40, 1), axis=0)
# 使用sin函数得到y值，并拉伸到一维
y = np.sin(X).ravel()
# y值增加噪声
y[::5] += 1 * (0.5 - np.random.rand(8))
#在图像上显示样本点，s是点的大小，label表示样本标签，一般显示在图像右上角
plt.scatter(X,y,marker='o',color='r',s=100,label='bad') 
</code></pre>
<p>显示如下：</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/1654135.png" alt="img"></p>
<p><strong>KNN 学习 - 预测</strong></p>
<pre class=" language-language-python"><code class="language-language-python">#建立模型，k近邻数=3，L2欧几里得距离
clf = KNeighborsRegressor(n_neighbors=3, p=2)
# 拟合
clf.fit(X, y)
# 预测
# 创建[0, 5]之间的500个数的等差数列, 作为测试数据
T = np.linspace(0, 5, 500)[:, np.newaxis]
#y_是预测值，y_real是数据的实际值
y_ = clf.predict(T)
y_real=np.sin(T).ravel()
plt.scatter(T,y_real,marker='o',color='r',s=1,label='bad') #在图像上显示样本点，s是点的大小，label表示样本标签，一般显示在图像右上角
plt.plot(T, y_) #根据X,Y画出描点图，其实主要作用是连线。X_pred和Y_pred一般为数组
</code></pre>
<p>如下图：</p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/156465.png" alt="img"></p>
<p><strong>关于 sklearn.neighbors.KNeighborsRegressor</strong></p>
<p>KNN 的模型应该是一张<strong>距离表</strong></p>
<pre class=" language-language-python"><code class="language-language-python">KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)
    """
    最近邻回归，用于预测连续数值
    :param n_neighbors: int, default=5。k值，最近的k个邻居
    :param weights: 
    :param algorithm: 寻找最近邻的算法
        ball_tree：will use BallTree。
        kd_tree： will use KDTree
        brute： 暴力穷举
        auto： 自动选择
    :param leaf_size: int, default=30。传递给BallTree或KDTree的叶大小。这会影响构造和查询的速度，以及存储树所需的内存。
    :param p: int, default=2。当p=1时，使用曼哈顿距离（l1）；对p=2时，欧几里德距离（l2）。对于任意p，使用minkowski距离（lp）。
    :param metric: str or callable, default=’minkowski’。要用于树的距离度量。默认的度量是minkowski，p=2等于标准的欧几里德度量。有关可用度量的列表
    :param metric_params: 
    :param n_jobs: int, default=None。为邻居搜索运行的并行作业数。
    :param kwargs: 
    :return: KNN模型
    """
</code></pre>
<blockquote>
<p>一般指定一个 k 就可以了</p>
</blockquote>
<pre class=" language-language-python"><code class="language-language-python">neigh = KNeighborsRegressor(n_neighbors=2)
</code></pre>
<p><strong>此外，关于近邻的遍历算法：</strong></p>
<ul>
<li><strong>KDTree：<strong>对于</strong>低维度</strong> (<img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/7fb5b8aaa79d55e35332a1f02a5aee04.jpg" alt="D &lt; 20">) 近邻搜索非常快，当 <img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/e03066df748abd9273db055cb79f0f01.jpg" alt="D"> 增长到很大时，效率变低。这就是所谓的 “维度灾难” 的一种体现。</li>
<li><strong>BallTree：<strong>比 KD 树消耗更多的时间，但是这种数据结构对于高结构化的数据是非常有效的，即使在</strong>高维度</strong>上也是一样.</li>
</ul>
<p>函数 <code>KNeighborsRegressor()</code>  返回一个 <code>KNeighborsRegressor</code>  对象。</p>
<p><strong>常用属性（成员变量）：</strong></p>
<ul>
<li><code>effective_metric_</code> ：<strong>str or callable</strong>。距离度量。如 <code>metric='minkowski'</code>  并且 <code>p=2</code>  时，返回 <code>euclidean-欧几里得</code></li>
<li><code>effective_metric_params_</code> ：<strong>dict</strong>。度量函数的其他参数</li>
<li><code>n_samples_fit_</code> ：<strong>int</strong>。用于训练的样本数</li>
</ul>
<p><strong>常用函数：</strong></p>
<ul>
<li>
<p><code>fit(X, y)</code> ：拟合（训练）模型。</p>
</li>
<li>
<p><code>predict(X)</code> ：预测</p>
</li>
<li>
<p><code>get_params(deep=True)</code> ：以字典形式返回模型参数，包括但不限于 <code>fit_intercept</code></p>
</li>
<li>
<p><code>set_params(**params)</code> ：设置参数</p>
</li>
<li>
<p><code>kneighbors(X=None, n_neighbors=None, return_distance=True)</code> ：返回指定样本的最近的 k 个邻居</p>
<ul>
<li><code>X</code> ：<strong>array</strong>。样本数组</li>
<li><code>n_neighbors</code> ：<strong>int</strong>。邻居数，默认是一开始传入的 k 值</li>
<li><code>return_distance</code> ：<strong>bool</strong>。是否返回距离。</li>
<li><strong>return</strong>：
<ul>
<li><code>neigh_dist</code> ：<strong>ndarray</strong>。各邻居的距离（需要 return_distance 为 true）。二维数组（一维 - 样本；二维 - 每个样本的邻居们）</li>
<li><code>neigh_ind </code> ：<strong>ndarray</strong>。邻居的索引。二维数组（一维 - 样本；二维 - 每个样本的邻居们）</li>
</ul>
</li>
</ul>
<p>例：</p>
<pre class=" language-language-python"><code class="language-language-python">>>> samples = [[0, 0, 0], [0, 0.5, 0], [1, 1, 0.5]]
>>> neigh = NearestNeighbors(n_neighbors=1)#NearestNeighbors是无监督的最近邻
>>> neigh.fit(samples)
>>> neigh.kneighbors([[1, 1, 1]])
(array([[0.5]]), array([[2]]))#表示最近的1个邻居距离为0.5，index=2
#或者这样接收也可
distances, indices = neigh.kneighbors([[1, 1, 1]])
>>> distances
array([[0.5]])#行-样本，列-样本的每个邻居的距离
>>> indices
array([[2]])#最近的1个邻居是index=2的点
</code></pre>
</li>
<li>
<p><code>kneighbors_graph(X=None, n_neighbors=None,mode='connectivity')</code> ：返回邻居<strong>连接图</strong>，1 - 相邻；0 - 不相邻</p>
<ul>
<li><code>X</code> ：<strong>array</strong>。样本数组</li>
<li><code>n_neighbors</code> ：<strong>int</strong>。邻居数，默认是一开始传入的 k 值</li>
<li><code>mode</code> ：返回矩阵的类型</li>
<li><strong>return</strong>：返回一个距离矩阵，可通过 <code>toArray()</code>  转为<strong> array</strong>。</li>
</ul>
<p>例：</p>
<pre class=" language-language-python"><code class="language-language-python">>>> X = [[0], [3], [1]]
>>> neigh = NearestNeighbors(n_neighbors=2).fit(X)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[1, 0, 1],#行-被测样本，这行是[0]。这行表示：对[0]来说，[0]和[1]是最近的2个邻居
       [0, 1, 1],#列-每个邻居（包括自身）
       [1, 0, 1]])
</code></pre>
</li>
</ul>
<h3 id="13-总结"><a class="anchor" href="#13-总结">#</a> 1.3 总结</h3>
<p>先计算出所有样本距离目标的<strong>距离</strong>，取最进的<strong> K 个</strong>样本的平均数 / 中位数 / 众数 /...</p>
<h2 id="2-knn-填充数据"><a class="anchor" href="#2-knn填充数据">#</a> 2. KNN 填充数据</h2>
<p>实际上，python 中的 KNNImputer 库很好的利用 knn 实现了数据填充，<strong>直接上原理：</strong></p>
<blockquote>
<p>对于数据缺失的特征点，计算与其他数据特征间的距离，选取 k 个最小距离的数据特征点，把这 k 个数据特征中对应于目标特征点数据缺失的地方进行求<strong>均值</strong>，作为填充数据。</p>
</blockquote>
<p><strong>举个例子：</strong></p>
<pre class=" language-language-text"><code class="language-language-text">X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]] = [n1 n2 n3 n4]
</code></pre>
<p><strong>含空值的欧式距离，如 n1 与 n3：</strong></p>
<p><img data-src="/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/equation.svg" alt="img"></p>
<blockquote>
<p>参考文献：John K. Dixon, “Pattern Recognition with Partly Missing Data”, IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue: 10, pp. 617 - 621, Oct. 1979.<span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHAlM0EvL2llZWV4cGxvcmUuaWVlZS5vcmcvYWJzdHJhY3QvZG9jdW1lbnQvNDMxMDA5MC8=">http://ieeexplore.ieee.org/abs</span></p>
</blockquote>
<p>对应 n1 与 n3 的距离为：</p>
<p>![img](%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/equation (1).svg)</p>
<p>对应 n1 与 n2 的距离为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msqrt><mrow><mfrac><mn>3</mn><mn>2</mn></mfrac><mo>×</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>3</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mn>2</mn><mo>−</mo><mn>4</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></msqrt><mo>=</mo><msqrt><mn>12</mn></msqrt><mo>=</mo><mn>3.464</mn></mrow><annotation encoding="application/x-tex">\sqrt{\frac{3}{2}\times ((1-3)^2+(2-4)^2)}=\sqrt{12}=3.464
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.44em;vertical-align:-0.788405em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.651595em;"><span class="svg-align" style="top:-4.4em;"><span class="pstrut" style="height:4.4em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">3</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">4</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.6115950000000003em;"><span class="pstrut" style="height:4.4em;"></span><span class="hide-tail" style="min-width:1.02em;height:2.48em;"><svg width="400em" height="2.48em" viewbox="0 0 400000 2592" preserveaspectratio="xMinYMin slice"><path d="M424,2478
c-1.3,-0.7,-38.5,-172,-111.5,-514c-73,-342,-109.8,-513.3,-110.5,-514
c0,-2,-10.7,14.3,-32,49c-4.7,7.3,-9.8,15.7,-15.5,25c-5.7,9.3,-9.8,16,-12.5,20
s-5,7,-5,7c-4,-3.3,-8.3,-7.7,-13,-13s-13,-13,-13,-13s76,-122,76,-122s77,-121,77,-121
s209,968,209,968c0,-2,84.7,-361.7,254,-1079c169.3,-717.3,254.7,-1077.7,256,-1081
l0 -0c4,-6.7,10,-10,18,-10 H400000
v40H1014.6
s-87.3,378.7,-272.6,1166c-185.3,787.3,-279.3,1182.3,-282,1185
c-2,6,-10,9,-24,9
c-8,0,-12,-0.7,-12,-2z M1001 80
h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.788405em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.08390500000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.956095em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">1</span><span class="mord">2</span></span></span><span style="top:-2.916095em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.08390500000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">.</span><span class="mord">4</span><span class="mord">6</span><span class="mord">4</span></span></span></span></span></p>
<p><strong>python 的 nan_euclidean_distances 函数可计算含空值的距离矩阵</strong>：</p>
<pre class=" language-language-python"><code class="language-language-python">from sklearn.metrics import nan_euclidean_distances
X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]
nan_euclidean_distances(X,X)
</code></pre>
<p>计算出的距离矩阵为：</p>
<pre class=" language-language-python"><code class="language-language-python">array([[ 0.        ,  3.46410162,  6.92820323, 11.29158979],
       [ 3.46410162,  0.        ,  3.46410162,  7.54983444],
       [ 6.92820323,  3.46410162,  0.        ,  3.46410162],
       [11.29158979,  7.54983444,  3.46410162,  0.        ]])
</code></pre>
<p><strong>python 可直接用 KNNImputer 进行空值填充：</strong></p>
<pre class=" language-language-python"><code class="language-language-python">from sklearn.impute import KNNImputer
X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]
imputer = KNNImputer(n_neighbors=2)
imputer.fit_transform(X)
</code></pre>
<p>KNNImputer 中可选择 k 值来进行数据填充，这里选择 2，填充结果：</p>
<pre class=" language-language-python"><code class="language-language-python">array([[1. , 2. , 4. ],
       [3. , 4. , 3. ],
       [5.5, 6. , 5. ],
       [8. , 8. , 7. ]])
</code></pre>

      <div class="tags">
          <a href="../../../../tags/%E7%AE%97%E6%B3%95/" rel="tag"><i class="ic i-tag"></i> 算法</a>
          <a href="../../../../tags/Python/" rel="tag"><i class="ic i-tag"></i> Python</a>
          <a href="../../../../tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 机器学习</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">更新于</span>
    <time title="修改时间：2021-08-31 19:37:46" itemprop="dateModified" datetime="2021-08-31T19:37:46+08:00">2021-08-31</time>
  </span>
</div>

      
<div class="reward">
  <button><i class="ic i-heartbeat"></i> 赞赏</button>
  <p>请我喝[茶]~(￣▽￣)~*</p>
  <div id="qr">
      
      <div>
        <img data-src="../images/wechatpay.png" alt="宁理大神1996 微信支付">
        <p>微信支付</p>
      </div>
      
      <div>
        <img data-src="../images/alipay.png" alt="宁理大神1996 支付宝">
        <p>支付宝</p>
      </div>
      
      <div>
        <img data-src="../images/paypal.png" alt="宁理大神1996 贝宝">
        <p>贝宝</p>
      </div>
  </div>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>本文作者： </strong>宁理大神1996 <i class="ic i-at"><em>@</em></i>宁理大神 1996
  </li>
  <li class="link">
    <strong>本文链接：</strong>
    <a href="../../../../https:/nitgod1996.com/2021/06/10/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%A4%8D%E4%B9%A0%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/" title="西瓜书复习及其部分代码实现">https://nitgod1996.com/2021/06/10/西瓜书复习及代码实现/</a>
  </li>
  <li class="license">
    <strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="../../07/%E7%A8%80%E7%96%8F%E6%95%B0%E6%8D%AE%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipeyonbf9j20zk0m8e81.jpg" title="降低数据稀疏度的算法研究">
  <span class="type">上一篇</span>
  <span class="category"><i class="ic i-flag"></i> </span>
  <h3>降低数据稀疏度的算法研究</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="../../21/note/Python/jupyter/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclh5u05ej20zk0m87df.jpg" title="jupty使用方法">
  <span class="type">下一篇</span>
  <span class="category"><i class="ic i-flag"></i> </span>
  <h3>jupty使用方法</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="文章目录">
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7"><span class="toc-number">1.</span> <span class="toc-text"> 数学符号</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E7%BB%AA%E8%AE%BA"><span class="toc-number">2.</span> <span class="toc-text"> 一、 绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD%E5%8F%8A%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.</span> <span class="toc-text"> 1. 基本术语及概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text"> 2. 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.1.</span> <span class="toc-text"> 2.1 阶跃函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.2.</span> <span class="toc-text"> 2.2 sigmoid 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-relu-%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.3.</span> <span class="toc-text"> 2.3 ReLU 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#24-softmax-%E5%87%BD%E6%95%B0%E6%9C%80%E5%90%8E%E4%B8%80%E5%B1%82%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.4.</span> <span class="toc-text"> 2.4 softmax 函数（最后一层输出层的激活函数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#25-%E5%87%BD%E6%95%B0%E5%9B%BE"><span class="toc-number">2.2.5.</span> <span class="toc-text"> 2.5 函数图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.3.</span> <span class="toc-text"> 3. 梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">2.4.</span> <span class="toc-text"> 梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">2.5.</span> <span class="toc-text"> 缺点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.6.</span> <span class="toc-text"> 代码实现：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.0.1.</span> <span class="toc-text"> 1. 定义目标函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%B1%82%E5%87%BA%E6%A2%AF%E5%BA%A6%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.0.2.</span> <span class="toc-text"> 2. 求出梯度函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.6.0.3.</span> <span class="toc-text"> 3. 实现梯度下降法</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-number">3.</span> <span class="toc-text"> 二、 模型评估与选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%AF%AF%E5%B7%AE%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">3.1.</span> <span class="toc-text"> 1. 误差与过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text"> 2. 评估方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text"> 三、 线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92linear-regression"><span class="toc-number">4.1.</span> <span class="toc-text"> 1. 线性回归（linear regression）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">4.1.1.</span> <span class="toc-text"> 1.1 算法原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#111-%E5%8D%95%E5%B1%9E%E6%80%A7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.1.1.1.</span> <span class="toc-text"> 1.1.1 单属性线性回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#112-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.1.1.2.</span> <span class="toc-text"> 1.1.2 多元线性回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-python-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.1.2.</span> <span class="toc-text"> 1.2 Python 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.2.</span> <span class="toc-text"> 2. 对数几率回归（逻辑回归）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.2.1.</span> <span class="toc-text"> 2.1 与线性回归的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-sigmod-%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.2.</span> <span class="toc-text"> 2.2 sigmod 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">4.2.3.</span> <span class="toc-text"> 2.3 算法原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#24-%E4%BB%A3%E7%A0%81"><span class="toc-number">4.2.4.</span> <span class="toc-text"> 2.4 代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">5.</span> <span class="toc-text"> 四、 决策树</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E9%98%B6"><span class="toc-number">6.</span> <span class="toc-text"> 五、 神经网络初阶</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text"> 1. 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.1.</span> <span class="toc-text"> 1.1 阶跃函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.2.</span> <span class="toc-text"> 1.2 sigmoid 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-relu-%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.3.</span> <span class="toc-text"> 1.3 ReLU 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-softmax-%E5%87%BD%E6%95%B0%E6%9C%80%E5%90%8E%E4%B8%80%E5%B1%82%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.4.</span> <span class="toc-text"> 1.4 softmax 函数（最后一层输出层的激活函数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-%E5%87%BD%E6%95%B0%E5%9B%BE"><span class="toc-number">6.1.5.</span> <span class="toc-text"> 1.5 函数图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B1%82%E6%AC%A1%E9%80%92%E8%BF%9B"><span class="toc-number">6.2.</span> <span class="toc-text"> 2. 神经网络的层次递进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.3.</span> <span class="toc-text"> 3. 神经网络的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1"><span class="toc-number">6.3.1.</span> <span class="toc-text"> 3.1 特征提取与算法设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">6.3.2.</span> <span class="toc-text"> 3.2 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#321-%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">6.3.2.1.</span> <span class="toc-text"> 3.2.1 均方误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#322-%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE"><span class="toc-number">6.3.2.2.</span> <span class="toc-text"> 3.2.2 交叉熵误差</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.3.3.</span> <span class="toc-text"> 3.3 神经网络学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="toc-number">6.3.4.</span> <span class="toc-text"> 3.4 误差反向传播求梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#341-%E5%85%B3%E4%BA%8E-affine-%E5%B1%82%E7%9A%84%E7%9F%A9%E9%98%B5%E5%8F%8D%E5%90%91%E6%B1%82%E5%AF%BC"><span class="toc-number">6.3.4.1.</span> <span class="toc-text"> 3.4.1 关于 Affine 层的矩阵反向求导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#342-%E6%9C%80%E4%B8%8A%E6%B8%B8%E5%AF%BC%E6%95%B0%E7%94%9F%E6%88%90softmax_with_loss%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9B%B8%E5%AF%B9%E4%BA%8E-softmax-%E7%9A%84%E8%BE%93%E5%87%BA-y-%E7%9A%84%E4%BC%AA%E6%A2%AF%E5%BA%A6%E7%9F%A9%E9%98%B5"><span class="toc-number">6.3.4.2.</span> <span class="toc-text"> 3.4.2 最上游导数生成，softmax_with_loss，损失函数相对于 softmax 的输出 y 的伪梯度矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#343-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E6%A2%AF%E5%BA%A6%E4%BB%A3%E7%A0%81%E5%85%A8%E8%B2%8C"><span class="toc-number">6.3.4.3.</span> <span class="toc-text"> 3.4.3 反向传播求梯度代码全貌</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E7%A0%81%E8%AE%BE%E8%AE%A1"><span class="toc-number">6.4.</span> <span class="toc-text"> 4. 神经网络的代码设计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-functionpy"><span class="toc-number">6.4.1.</span> <span class="toc-text"> 4.1 function.py</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#411-%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">6.4.1.1.</span> <span class="toc-text"> 4.1.1 正向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#412-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">6.4.1.2.</span> <span class="toc-text"> 4.1.2 反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#413-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">6.4.1.3.</span> <span class="toc-text"> 4.1.3 损失函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-layerpy-%E5%90%84%E5%B1%82%E7%9A%84%E7%B1%BB"><span class="toc-number">6.4.2.</span> <span class="toc-text"> 4.2 layer.py 各层的类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#421-relu-%E5%B1%82"><span class="toc-number">6.4.2.1.</span> <span class="toc-text"> 4.2.1 ReLU 层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#422-sigmoid-%E5%B1%82"><span class="toc-number">6.4.2.2.</span> <span class="toc-text"> 4.2.2 sigmoid 层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#423-affine-%E5%B1%82%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%B9%9F%E5%B0%B1%E6%98%AF-awxb-%E9%82%A3%E5%B1%82"><span class="toc-number">6.4.2.3.</span> <span class="toc-text"> 4.2.3 Affine 层（全连接层，也就是 a&#x3D;wx+b 那层）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#424-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">6.4.2.4.</span> <span class="toc-text"> 4.2.4 卷积层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#425-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">6.4.2.5.</span> <span class="toc-text"> 4.2.5 池化层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#426-softmax-%E8%87%B3%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%B1%82"><span class="toc-number">6.4.2.6.</span> <span class="toc-text"> 4.2.6 softmax 至损失函数层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-neuralnet-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B1%BB"><span class="toc-number">6.4.3.</span> <span class="toc-text"> 4.3 Neuralnet 神经网络类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#431-%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-number">6.4.3.1.</span> <span class="toc-text"> 4.3.1 类变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#432-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E5%AE%9A%E4%B9%89%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-number">6.4.3.2.</span> <span class="toc-text"> 4.3.2 初始化函数（主要用于定义类变量）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#433-%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">6.4.3.3.</span> <span class="toc-text"> 4.3.3 预测函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#434-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">6.4.3.4.</span> <span class="toc-text"> 4.3.4 损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#435-%E8%AE%A1%E7%AE%97%E7%B2%BE%E5%BA%A6"><span class="toc-number">6.4.3.5.</span> <span class="toc-text"> 4.3.5 计算精度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#436-%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="toc-number">6.4.3.6.</span> <span class="toc-text"> 4.3.6 求梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#437-%E4%BF%9D%E5%AD%98%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E5%AE%8C%E5%90%8E%E4%BF%9D%E5%AD%98%E4%B8%8B%E6%9C%80%E7%BB%88%E6%9D%83%E9%87%8D%E7%AD%89%E5%8F%82%E6%95%B0"><span class="toc-number">6.4.3.7.</span> <span class="toc-text"> 4.3.7 保存参数（学习完后保存下最终权重等参数）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#438-%E5%8A%A0%E8%BD%BD%E5%8F%82%E6%95%B0%E9%A2%84%E6%B5%8B%E6%97%B6%E5%8A%A0%E8%BD%BD%E4%B9%8B%E5%89%8D%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-number">6.4.3.8.</span> <span class="toc-text"> 4.3.8 加载参数（预测时加载之前学习到的参数）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-trainer-%E8%AE%AD%E7%BB%83%E7%B1%BB%E5%AD%A6%E4%B9%A0%E7%B1%BB"><span class="toc-number">6.4.4.</span> <span class="toc-text"> 4.4 Trainer 训练类（学习类）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#441-%E7%B1%BB%E5%8F%98%E9%87%8F%E5%8F%8A%E5%88%9D%E5%A7%8B%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-number">6.4.4.1.</span> <span class="toc-text"> 4.4.1 类变量及初始化函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#442-%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">6.4.4.2.</span> <span class="toc-text"> 4.4.2 训练函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#443-%E6%9C%80%E7%BB%88%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0%E6%8F%90%E4%BE%9B%E7%BB%99%E5%A4%96%E7%95%8C%E7%9A%84-api"><span class="toc-number">6.4.4.3.</span> <span class="toc-text"> 4.4.3 最终训练函数（提供给外界的 API）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#45-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD-mnistpy"><span class="toc-number">6.4.5.</span> <span class="toc-text"> 4.5 数据集加载 mnist.py</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#451-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%89%80%E9%9C%80%E5%8F%82%E6%95%B0"><span class="toc-number">6.4.5.1.</span> <span class="toc-text"> 4.5.1 初始化所需参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#452-%E4%B8%8B%E8%BD%BD%E6%89%80%E9%9C%80%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.4.5.2.</span> <span class="toc-text"> 4.5.2 下载所需数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#453-%E8%A7%A3%E5%8E%8B%E6%89%93%E5%BC%80-gzip-%E6%96%87%E4%BB%B6"><span class="toc-number">6.4.5.3.</span> <span class="toc-text"> 4.5.3 解压打开 gzip 文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#454-%E5%B0%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BF%A1%E6%81%AF%E4%BF%9D%E5%AD%98%E5%9C%A8-dict-%E4%B8%AD"><span class="toc-number">6.4.5.4.</span> <span class="toc-text"> 4.5.4 将数据集信息保存在 dict 中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#455-%E5%B0%86%E8%A7%A3%E5%8E%8B%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BF%9D%E5%AD%98%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-number">6.4.5.5.</span> <span class="toc-text"> 4.5.5 将解压后的数据集保存到本地</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#456-%E7%9B%91%E7%9D%A3%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2"><span class="toc-number">6.4.5.6.</span> <span class="toc-text"> 4.5.6 监督数据格式转换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#457-%E5%AF%B9%E5%A4%96%E6%8E%A5%E5%8F%A3-api"><span class="toc-number">6.4.5.7.</span> <span class="toc-text"> 4.5.7 对外接口 api</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#46-%E6%89%A7%E8%A1%8C%E4%B8%BB%E5%87%BD%E6%95%B0%E5%8C%85%E6%8B%AC%E5%9B%BE%E5%83%8F%E6%98%BE%E7%A4%BA"><span class="toc-number">6.4.6.</span> <span class="toc-text"> 4.6 执行主函数（包括图像显示）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#461-%E8%AF%BB%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">6.4.6.1.</span> <span class="toc-text"> 4.6.1 读入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#462-%E5%88%9B%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E8%B1%A1"><span class="toc-number">6.4.6.2.</span> <span class="toc-text"> 4.6.2 创建神经网络对象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#463-%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E5%AF%B9%E8%B1%A1"><span class="toc-number">6.4.6.3.</span> <span class="toc-text"> 4.6.3 创建训练对象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#464-%E8%AE%AD%E7%BB%83%E5%B9%B6%E4%BF%9D%E5%AD%98%E5%8F%82%E6%95%B0"><span class="toc-number">6.4.6.4.</span> <span class="toc-text"> 4.6.4 训练并保存参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#465-%E7%BB%98%E5%9B%BE"><span class="toc-number">6.4.6.5.</span> <span class="toc-text"> 4.6.5 绘图</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-number">6.5.</span> <span class="toc-text"> 5. 总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-rnn-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text"> 五 +、 RNN 循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-number">7.1.</span> <span class="toc-text"> 解决梯度爆炸和梯度消失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-lstm-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86"><span class="toc-number">7.2.</span> <span class="toc-text"> 2. LSTM 长短时记忆</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">8.</span> <span class="toc-text"> 六、 支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">8.1.</span> <span class="toc-text"> 正则化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81-knn-%E7%AE%97%E6%B3%95"><span class="toc-number">9.</span> <span class="toc-text"> 十、 KNN 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-knn-%E7%AE%97%E6%B3%95"><span class="toc-number">9.1.</span> <span class="toc-text"> 1. KNN 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E6%AD%A5%E9%AA%A4"><span class="toc-number">9.1.1.</span> <span class="toc-text"> 1.1 步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.1.2.</span> <span class="toc-text"> 1.2 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-%E6%80%BB%E7%BB%93"><span class="toc-number">9.1.3.</span> <span class="toc-text"> 1.3 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-knn-%E5%A1%AB%E5%85%85%E6%95%B0%E6%8D%AE"><span class="toc-number">9.2.</span> <span class="toc-text"> 2. KNN 填充数据</span></a></li></ol></li></ol>
      </div>
      <div class="related panel pjax" data-title="系列文章">
        <ul>
          <li><a href="../../../04/23/note/%E5%89%8D%E7%AB%AF/echarts/" rel="bookmark" title="echarts用法简单记录">echarts用法简单记录</a></li><li><a href="../../../05/10/note/software/maven%E5%A4%8D%E4%B9%A0/" rel="bookmark" title="maven复习">maven复习</a></li><li><a href="../../../05/10/note/hexo/Hexo%E6%90%AD%E5%BB%BAGitHub%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/" rel="bookmark" title="Hexo搭建GitHub静态博客">Hexo搭建GitHub静态博客</a></li><li><a href="../../../05/12/note/Linux/VMware%E5%AE%89%E8%A3%85%E4%B8%AD%E6%A0%87%E9%BA%92%E9%BA%9F/" rel="bookmark" title="VMware安装中标麒麟">VMware安装中标麒麟</a></li><li><a href="../../../05/13/note/Python/pandas/" rel="bookmark" title="Python机器学习库">Python机器学习库</a></li><li><a href="../../../05/17/note/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-Java%E8%B0%83%E7%94%A8Python%E8%84%9A%E6%9C%AC/" rel="bookmark" title="随笔-Java调用Python脚本">随笔-Java调用Python脚本</a></li><li><a href="../../../05/18/note/Java/java%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E5%8F%8A%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/" rel="bookmark" title="java基础语法及特性">java基础语法及特性</a></li><li><a href="../../../05/20/note/%E9%9A%8F%E7%AC%94/%E5%87%86%E5%A4%87%E5%AD%A6%E4%B9%A0docker/" rel="bookmark" title="docker随手记">docker随手记</a></li><li><a href="../../../05/23/note/%E5%89%8D%E7%AB%AF/CSS%EF%BC%88%E5%BC%83%E7%94%A8%EF%BC%89/" rel="bookmark" title="CSS笔记及参考手册">CSS笔记及参考手册</a></li><li><a href="../../../05/26/note/%E5%89%8D%E7%AB%AF/html%EF%BC%88%E5%BC%83%E7%94%A8%EF%BC%89/" rel="bookmark" title="html常用功能手册">html常用功能手册</a></li><li><a href="../../../05/27/note/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-%E5%85%B3%E4%BA%8E%E5%89%8D%E7%AB%AF%E8%B0%83%E5%8F%96Python%E6%95%B0%E6%8D%AE/" rel="bookmark" title="前端调用Python数据">前端调用Python数据</a></li><li><a href="../../../05/27/note/%E5%89%8D%E7%AB%AF/JavaScript/" rel="bookmark" title="JavaScript基础语法">JavaScript基础语法</a></li><li><a href="../../../05/27/note/%E5%89%8D%E7%AB%AF/jQuery/" rel="bookmark" title="jQuery笔记">jQuery笔记</a></li><li><a href="../../../05/28/note/%E9%9A%8F%E7%AC%94/Python/%E9%9A%8F%E7%AC%94-%E5%88%A9%E7%94%A8Python%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86%E6%96%87%E4%BB%B6/" rel="bookmark" title="随笔-利用Python批量处理文件">随笔-利用Python批量处理文件</a></li><li><a href="../../../05/29/note/%E9%9A%8F%E7%AC%94/Python/%E9%9A%8F%E7%AC%94-%E5%88%A9%E7%94%A8Python%E5%A4%84%E7%90%86json/" rel="bookmark" title="随笔-利用Python处理json">随笔-利用Python处理json</a></li><li><a href="../../02/note/%E9%9A%8F%E7%AC%94/%E6%AF%8F%E6%97%A5%E4%B8%80%E7%AC%94/%E5%9F%BA%E4%BA%8EPython%E5%92%8Cecharts%E7%9A%84%E5%8A%A8%E6%80%81%E5%9B%BE/" rel="bookmark" title="每日一笔-基于Python和echarts的动态图">每日一笔-基于Python和echarts的动态图</a></li><li><a href="../../04/note/Python/Python%E8%AF%AD%E6%B3%95%E5%A4%8D%E4%B9%A0/" rel="bookmark" title="Python语法复习">Python语法复习</a></li><li class="active"><a href="" rel="bookmark" title="西瓜书复习及其部分代码实现">西瓜书复习及其部分代码实现</a></li>
        </ul>
      </div>
      <div class="overview panel" data-title="站点概览">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="宁理大神1996"
      data-src="../images/avatar.jpg">
  <p class="name" itemprop="name">宁理大神1996</p>
  <div class="description" itemprop="description">宁理大神的个人博客</div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="../archives/">
        <span class="count">50</span>
        <span class="name">文章</span>
      </a>
    </div>
    <div class="item categories">
      <a href="../categories/">
        <span class="count">14</span>
        <span class="name">分类</span>
      </a>
    </div>
    <div class="item tags">
      <a href="../tags/">
        <span class="count">22</span>
        <span class="name">标签</span>
      </a>
    </div>
</nav>

<div class="social">
      <span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lvdXJuYW1l" title="https:&#x2F;&#x2F;github.com&#x2F;yourname"><i class="ic i-github"></i></span>
      <span class="exturl item twitter" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;twitter.com&#x2F;yourname"><i class="ic i-twitter"></i></span>
      <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yourname"><i class="ic i-zhihu"></i></span>
      <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPXlvdXJpZA==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;yourid"><i class="ic i-cloud-music"></i></span>
      <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20veW91cm5hbWU=" title="https:&#x2F;&#x2F;weibo.com&#x2F;yourname"><i class="ic i-weibo"></i></span>
      <span class="exturl item about" data-url="aHR0cHM6Ly9hYm91dC5tZS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;about.me&#x2F;yourname"><i class="ic i-address-card"></i></span>
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="../index.html" rel="section"><i class="ic i-home"></i>首页</a>
  </li>

    
  <li class="item">
    <a href="../about/" rel="section"><i class="ic i-user"></i>关于</a>
  </li>

    
  <li class="item">
    <a href="../archives/" rel="section"><i class="ic i-archive"></i>归档</a>
  </li>

    
  <li class="item">
    <a href="../categories/" rel="section"><i class="ic i-th"></i>分类</a>
  </li>

    
  <li class="item">
    <a href="../tags/" rel="section"><i class="ic i-tags"></i>标签</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="../../07/%E7%A8%80%E7%96%8F%E6%95%B0%E6%8D%AE%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="../../21/note/Python/jupyter/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>随机文章</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="../../../../2022/04/11/markdown%E8%AF%AD%E6%B3%95/" title="markdown语法">markdown语法</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="../../../../2022/08/18/note/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%85%AB%E8%82%A1/" title="操作系统八股">操作系统八股</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="../../../../categories/note/" title="分类于 笔记">笔记</a>
</div>

    <span><a href="" title="西瓜书复习及其部分代码实现">西瓜书复习及其部分代码实现</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="../../../../2022/08/15/note/CSS/CSS%E5%85%AB%E8%82%A1/" title="CSS八股">CSS八股</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="../../21/note/Python/jupyter/" title="jupty使用方法">jupty使用方法</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="../../../05/14/%E9%99%88%E5%BA%B7%E4%B8%9C%E5%92%8C%E7%8E%8B%E5%86%B0%E5%86%B0%E7%9A%84%E5%BF%AB%E4%B9%90%E5%B0%8F%E5%B1%8B/" title="陈康东和王冰冰的快乐小屋">陈康东和王冰冰的快乐小屋</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="../../../../categories/note/" title="分类于 笔记">笔记</a>
<i class="ic i-angle-right"></i>
<a href="../../../../categories/note/java/" title="分类于 Java">Java</a>
</div>

    <span><a href="../../../05/18/note/Java/java%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E5%8F%8A%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/" title="java基础语法及特性">java基础语法及特性</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="../../../../categories/computer-science/" title="分类于 计算机科学">计算机科学</a>
<i class="ic i-angle-right"></i>
<a href="../../../../categories/computer-science/java/" title="分类于 Java">Java</a>
<i class="ic i-angle-right"></i>
<a href="../../../../categories/computer-science/java/course-1/" title="分类于 零基础学 Java 语言 - 浙江大学 - 翁恺">零基础学 Java 语言 - 浙江大学 - 翁恺</a>
</div>

    <span><a href="../../../05/09/test/" title="my hexo">my hexo</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="../../../../categories/note/" title="分类于 笔记">笔记</a>
<i class="ic i-angle-right"></i>
<a href="../../../../categories/note/%E9%9A%8F%E7%AC%94/" title="分类于 随笔">随笔</a>
</div>

    <span><a href="../../../05/20/note/%E9%9A%8F%E7%AC%94/%E5%87%86%E5%A4%87%E5%AD%A6%E4%B9%A0docker/" title="docker随手记">docker随手记</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
</div>

    <span><a href="../../../../2022/08/28/note/JavaScript/JavaScript%E5%85%AB%E8%82%A1/" title="JavaScript八股文">JavaScript八股文</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>最新评论</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2022</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">宁理大神1996 @ nitgod1996</span>
  </div>
  <div class="count">
    <span class="post-meta-item-icon">
      <i class="ic i-chart-area"></i>
    </span>
    <span title="站点总字数">566k 字</span>

    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="ic i-coffee"></i>
    </span>
    <span title="站点阅读时长">8:35</span>
  </div>
  <div class="powered-by">
    基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2021/06/10/西瓜书复习及代码实现/',
    favicon: {
      show: "（●´3｀●）やれやれだぜ",
      hide: "(´Д｀)大変だ！"
    },
    search : {
      placeholder: "文章搜索",
      empty: "关于 「 ${query} 」，什么也没搜到",
      stats: "${time} ms 内找到 ${hits} 条结果"
    },
    valine: true,fancybox: true,copyright: '复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>


<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>


<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="../../../../js/app.js?v=0.2.5"></script>




<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
